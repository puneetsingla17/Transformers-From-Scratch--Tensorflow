{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import glob\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../input/news-summary/news_summary_more.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.sample(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=df1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines=df1.headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text=text.lower()\n",
    "    text=text.strip()\n",
    "    #text=text.replace(\"'s\",\"\",text)\n",
    "    text=re.sub(r'[^\\w\\d]',\" \",text)\n",
    "    text=re.sub(r'\\d+','num',text)\n",
    "    text=re.sub(r\"ยน\",\"\",text)\n",
    "    text=re.sub(r\"\\s\\w\\s\",\" \",text)\n",
    "    text=re.sub(r\"\\s{2,}\",\" \",text)\n",
    "    text=text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processtext=text.apply(lambda x:preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processheadline=headlines.apply(lambda x:preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "textwordcount=Counter()\n",
    "for i in range(len(processtext)):\n",
    "    textwordcount.update(processtext.iloc[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlinewordcount=Counter()\n",
    "for i in range(len(processheadline)):\n",
    "    headlinewordcount.update(processheadline.iloc[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "textokenizer=Tokenizer(num_words=30000,oov_token='<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i,j in textokenizer.word_counts.items() if j>=3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "textokenizer.fit_on_texts(processtext)\n",
    "texttoken=textokenizer.texts_to_sequences(processtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "textwordind=textokenizer.word_index\n",
    "textwordind=dict([(i,j) for i,j in textwordind.items() if j<=30000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "textwordind['<pad>']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(textwordind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "headtokenizer=Tokenizer(num_words=16500,oov_token='<unk>')\n",
    "headtokenizer.fit_on_texts(processheadline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16207"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i,j in headtokenizer.word_counts.items() if j>=2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "headwordind=headtokenizer.word_index\n",
    "headwordind=dict([(i,j) for i,j in headwordind.items() if j<=16500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "headwordind['<pad>']=0\n",
    "headwordind['<start>']=len(headwordind)\n",
    "headwordind['<end>']=len(headwordind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headwordind['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlinetoken=headtokenizer.texts_to_sequences(processheadline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlinetokenin=[[headwordind['<start>']]+i for i in headlinetoken]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlinetokenout=[i+[headwordind['<end>']] for i in headlinetoken]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[16501, 2, 1238, 6, 517, 493, 95, 94, 6, 2417, 9169],\n",
       " [16501, 6863, 641, 2, 535, 3, 1263, 2019, 382, 84, 800, 6, 2],\n",
       " [16501, 641, 54, 3, 69, 611, 2060, 4, 1049, 303, 206, 5455],\n",
       " [16501, 47, 125, 368, 1733, 2, 2, 81, 3, 502, 44, 1280, 1060, 7, 1803]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlinetokenin[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "texttokenpad=pad_sequences(texttoken,padding='post')\n",
    "headlineinpad=pad_sequences(headlinetokenin,padding='post')\n",
    "headlineoutpad=pad_sequences(headlinetokenout,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxtextlen=texttokenpad.shape[1]\n",
    "maxsumlen=headlineinpad.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 77)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texttokenpad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxtextlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxsumlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=3\n",
    "numlayers=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiheadattention(modelsize,h):\n",
    "    querysize=modelsize//h  # querysize and valuesize are to be same \n",
    "    valuesize=modelsize//h\n",
    "    keysize=modelsize//h\n",
    "    wq=[tf.keras.layers.Dense(querysize,activation='relu') for _ in range(h)]\n",
    "    wk=[tf.keras.layers.Dense(querysize,activation='relu') for _ in range(h)]\n",
    "    wv=[tf.keras.layers.Dense(querysize,activation='relu') for _ in range(h)]\n",
    "    wo=tf.keras.layers.Dense(modelsize)\n",
    "    return wq,wk,wv,wo,keysize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(query,value,keysize,wq,wk,wv,wo,h):\n",
    "    # query len and keylen can be different size like in decoder but in encoder they are of same length\n",
    "    # they are same only in encoder but for decoder they will be different\n",
    "    # keylen is same as value len in decoder\n",
    "    heads=[]\n",
    "    for i in range(h):\n",
    "        score=tf.matmul(wq[i](query),wk[i](value),transpose_b=True)\n",
    "        score/=tf.math.sqrt(tf.dtypes.cast(keysize,tf.float32)) # batch x querylen x keylen\n",
    "        alignment=tf.nn.softmax(score,axis=2)\n",
    "        head=tf.matmul(alignment,wv[i](value))\n",
    "        heads.append(head)\n",
    "    \n",
    "    heads=tf.concat(heads,axis=2) # adds up valuesize dims which here is 64 across 2 dim to get 128\n",
    "    # batch x valuelen x 128\n",
    "    heads=wo(heads)\n",
    "    return heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_embedding(pos, model_size):\n",
    "    PE = np.zeros((1, model_size))\n",
    "    for i in range(model_size):\n",
    "        if i % 2 == 0:\n",
    "            PE[:,i] = np.sin(pos / 10000 ** (i / model_size))\n",
    "        else:\n",
    "            PE[:,i] = np.cos(pos / 10000 ** ((i - 1) / model_size))\n",
    "    return PE\n",
    "\n",
    "# max_length = max(len(data_en[0]), len(data_fr_in[0]))\n",
    "modelsize = 196\n",
    "\n",
    "pes = []\n",
    "for i in range(maxtextlen):\n",
    "    pes.append(positional_embedding(i, modelsize))\n",
    "\n",
    "pes = np.concatenate(pes, axis=0)\n",
    "pes = tf.constant(pes, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.int32,shape=[None,maxtextlen])\n",
    "yin=tf.placeholder(tf.int32,shape=[None,maxsumlen])\n",
    "yout=tf.placeholder(tf.int32,shape=[None,maxsumlen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const:0' shape=(77, 196) dtype=float32>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder:\n",
    "    def __init__(self,modelsize,maxtexlen,h):\n",
    "        \n",
    "        self.modelsize=modelsize\n",
    "        self.maxtexlen=maxtexlen\n",
    "        self.h=h\n",
    "        self.embedvar=tf.keras.layers.Embedding(len(textwordind),modelsize)\n",
    "        \n",
    "        self.wq1,self.wk1,self.wv1,self.wo1,self.keysize1=multiheadattention(self.modelsize,self.h)\n",
    "        self.d1=tf.keras.layers.Dense(4*modelsize,activation='relu')\n",
    "        self.d11=tf.keras.layers.Dense(modelsize)\n",
    "        self.batchnorm1=tf.keras.layers.BatchNormalization()\n",
    "        self.batchnorm11=tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.wq2,self.wk2,self.wv2,self.wo2,self.keysize2=multiheadattention(self.modelsize,self.h)\n",
    "        self.d2=tf.keras.layers.Dense(4*modelsize,activation='relu')\n",
    "        self.d21=tf.keras.layers.Dense(modelsize)\n",
    "        self.batchnorm2=tf.keras.layers.BatchNormalization()\n",
    "        self.batchnorm21=tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.wq3,self.wk3,self.wv3,self.wo3,self.keysize3=multiheadattention(self.modelsize,self.h)\n",
    "        self.d3=tf.keras.layers.Dense(4*modelsize,activation='relu')\n",
    "        self.d31=tf.keras.layers.Dense(modelsize)\n",
    "        self.batchnorm3=tf.keras.layers.BatchNormalization()\n",
    "        self.batchnorm31=tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "    def call(self,xinp):\n",
    "        xembed=self.embedvar(xinp) # batch x textlen x modelsize\n",
    "        \n",
    "        sub1=[]\n",
    "        for i in range(xembed.shape[1]):\n",
    "            sub=xembed[:,i,:]+pes[i,:]\n",
    "            sub1.append(tf.expand_dims(sub,axis=1))\n",
    "        \n",
    "        sub1=tf.concat(sub1,axis=1)\n",
    "        \n",
    "        sub11=call(sub1,sub1,self.keysize1,self.wq1,self.wk1,self.wv1,self.wo1,self.h)\n",
    "        sub11=sub11+sub1\n",
    "        subout=self.batchnorm1(sub11)\n",
    "        subout=self.d1(subout)\n",
    "        subout=self.d11(subout)\n",
    "        sub2=sub11+subout\n",
    "        sub2=self.batchnorm11(sub2)\n",
    "        \n",
    "        sub21=call(sub2,sub2,self.keysize2,self.wq2,self.wk2,self.wv2,self.wo2,self.h)\n",
    "        sub21=sub21+sub2\n",
    "        subout2=self.batchnorm2(sub21)\n",
    "        subout2=self.d2(subout2)\n",
    "        subout2=self.d21(subout2)\n",
    "        sub3=sub21+subout2\n",
    "        sub3=self.batchnorm21(sub3)\n",
    "        \n",
    "        sub31=call(sub3,sub3,self.keysize3,self.wq3,self.wk3,self.wv3,self.wo3,self.h)\n",
    "        sub31=sub31+sub3\n",
    "        subout3=self.batchnorm3(sub31)\n",
    "        subout3=self.d3(subout3)\n",
    "        subout3=self.d31(subout3)\n",
    "        sub4=sub31+subout3\n",
    "        sub4=self.batchnorm31(sub4)\n",
    "        return sub4\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder1=encoder(modelsize,maxtextlen,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderoutput=encoder1.call(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'batch_normalization_5/batchnorm/add_1:0' shape=(?, 77, 196) dtype=float32>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoderoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder:\n",
    "    def __init__(self,modelsize,h):\n",
    "        self.modelsize=modelsize\n",
    "        self.h=h\n",
    "        self.maxsumlen=maxsumlen\n",
    "        self.deembedvar=tf.keras.layers.Embedding(len(headwordind),modelsize)\n",
    "        \n",
    "        self.dwq1,self.dwk1,self.dwv1,self.dwo1,self.dkeysize1=multiheadattention(self.modelsize,self.h)\n",
    "        self.dd1=tf.keras.layers.Dense(4*modelsize,activation='relu')\n",
    "        self.dd11=tf.keras.layers.Dense(modelsize)\n",
    "        self.dbatchnorm1=tf.keras.layers.BatchNormalization()\n",
    "        self.dbatchnorm11=tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.dwq2,self.dwk2,self.dwv2,self.dwo2,self.dkeysize2=multiheadattention(self.modelsize,self.h)\n",
    "        self.dd2=tf.keras.layers.Dense(4*modelsize,activation='relu')\n",
    "        self.dd21=tf.keras.layers.Dense(modelsize)\n",
    "        self.dbatchnorm2=tf.keras.layers.BatchNormalization()\n",
    "        self.dbatchnorm21=tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.dwq3,self.dwk3,self.dwv3,self.dwo3,self.dkeysize3=multiheadattention(self.modelsize,self.h)\n",
    "        self.dd3=tf.keras.layers.Dense(4*modelsize,activation='relu')\n",
    "        self.dd31=tf.keras.layers.Dense(modelsize)\n",
    "        self.dbatchnorm3=tf.keras.layers.BatchNormalization()\n",
    "        self.dbatchnorm31=tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.den=tf.keras.layers.Dense(len(headwordind))\n",
    "        \n",
    "    def call(self,enout,yinput):\n",
    "        \n",
    "        deemb=self.deembedvar(yinput)\n",
    "        \n",
    "        sub=[]\n",
    "        for i in range(deemb.shape[1]):\n",
    "            s=call(tf.expand_dims(deemb[:,i,:],axis=1),deemb[:,:i,:],self.dkeysize1,self.dwq1,self.dwk1,self.dwv1,self.dwo1,self.h)\n",
    "            sub.append(s)\n",
    "        sub1=tf.concat(sub,axis=1)\n",
    "        sub1=sub1+deemb\n",
    "        sub1=self.dbatchnorm1(sub1)\n",
    "        \n",
    "        midsub=[]\n",
    "        for i in range(sub1.shape[1]):\n",
    "            s=call(tf.expand_dims(sub1[:,i,:],axis=1),enout,self.dkeysize2,self.dwq2,self.dwk2,self.dwv2,self.dwo2,self.h)\n",
    "            midsub.append(s)\n",
    "        sub2=tf.concat(midsub,axis=1)\n",
    "        sub2=sub1+sub2\n",
    "        sub2=self.dbatchnorm2(sub2)\n",
    "        sub21=self.dd2(sub2)\n",
    "        sub21=self.dd21(sub21)\n",
    "        sub21=sub21+sub2\n",
    "        sub3=self.dbatchnorm21(sub21)\n",
    "        \n",
    "        midsub1=[]\n",
    "        for i in range(sub3.shape[1]):\n",
    "            s=call(tf.expand_dims(sub3[:,i,:],axis=1),enout,self.dkeysize3,self.dwq3,self.dwk3,self.dwv3,self.dwo3,self.h)\n",
    "            midsub1.append(s)\n",
    "        sub4=tf.concat(midsub1,axis=1)\n",
    "        sub4=sub4+sub3\n",
    "        sub41=self.dbatchnorm3(sub4)\n",
    "        sub41=self.dd3(sub41)\n",
    "        sub41=self.dd31(sub41)\n",
    "        sub41=sub41+sub4\n",
    "        sub41=self.dbatchnorm31(sub41)\n",
    "        logits=self.den(sub41)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder2=decoder(modelsize,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16503"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(headwordind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(?, 17) dtype=int32>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits=decoder2.call(encoderoutput,yin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(17), Dimension(16503)])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=tf.cast(tf.not_equal(yout,0),tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1=tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=tf.one_hot(yout,len(headwordind)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1=loss1*mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=tf.reduce_mean(loss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainoptimizer=tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "trainstep=trainoptimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "trainloss=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:- 20  and loss is :- 4.7327266\n",
      "Epoch:- 40  and loss is :- 4.6317186\n",
      "Epoch:- 60  and loss is :- 4.5747433\n",
      "Epoch:- 80  and loss is :- 4.4879255\n",
      "Epoch:- 100  and loss is :- 4.396972\n",
      "Epoch:- 120  and loss is :- 4.5386953\n",
      "Epoch:- 140  and loss is :- 4.406602\n",
      "Epoch:- 160  and loss is :- 4.5059314\n",
      "Epoch:- 180  and loss is :- 4.4038925\n",
      "Epoch:- 200  and loss is :- 4.2837367\n",
      "Epoch:- 220  and loss is :- 4.29807\n",
      "Epoch:- 240  and loss is :- 4.1530757\n",
      "Epoch:- 260  and loss is :- 4.2197747\n",
      "Epoch:- 280  and loss is :- 4.168509\n",
      "Epoch:- 300  and loss is :- 4.0657268\n",
      "Epoch:- 320  and loss is :- 4.099449\n",
      "Epoch:- 340  and loss is :- 4.0648804\n",
      "Epoch:- 360  and loss is :- 4.0704503\n",
      "Epoch:- 380  and loss is :- 3.998827\n",
      "Epoch:- 400  and loss is :- 3.9947402\n",
      "Epoch:- 420  and loss is :- 3.9858842\n",
      "Epoch:- 440  and loss is :- 3.8425026\n",
      "Epoch:- 460  and loss is :- 3.937625\n",
      "Epoch:- 480  and loss is :- 3.9784696\n",
      "Epoch:- 500  and loss is :- 3.868476\n",
      "Epoch:- 520  and loss is :- 3.9594128\n",
      "Epoch:- 540  and loss is :- 3.8742173\n",
      "Epoch:- 560  and loss is :- 3.8211749\n",
      "Epoch:- 580  and loss is :- 3.9647863\n",
      "Epoch:- 600  and loss is :- 3.7899039\n",
      "Epoch:- 620  and loss is :- 3.603018\n",
      "Epoch:- 640  and loss is :- 3.7153146\n",
      "Epoch:- 660  and loss is :- 3.6060698\n",
      "Epoch:- 680  and loss is :- 3.5657709\n",
      "Epoch:- 700  and loss is :- 3.5726721\n",
      "Epoch:- 720  and loss is :- 3.603221\n",
      "Epoch:- 740  and loss is :- 3.48074\n",
      "Epoch:- 760  and loss is :- 3.614908\n",
      "Epoch:- 780  and loss is :- 3.5228808\n",
      "Epoch:- 800  and loss is :- 3.5107687\n",
      "Epoch:- 820  and loss is :- 3.4665847\n",
      "Epoch:- 840  and loss is :- 3.570024\n",
      "Epoch:- 860  and loss is :- 3.4868476\n",
      "Epoch:- 880  and loss is :- 3.438509\n",
      "Epoch:- 900  and loss is :- 3.6130471\n",
      "Epoch:- 920  and loss is :- 3.2181127\n",
      "Epoch:- 940  and loss is :- 3.120469\n",
      "Epoch:- 960  and loss is :- 3.2311785\n",
      "Epoch:- 980  and loss is :- 3.0861309\n",
      "Epoch:- 1000  and loss is :- 3.1742206\n",
      "Epoch:- 1020  and loss is :- 3.3342996\n",
      "Epoch:- 1040  and loss is :- 3.3469884\n",
      "Epoch:- 1060  and loss is :- 3.1794658\n",
      "Epoch:- 1080  and loss is :- 3.1299105\n",
      "Epoch:- 1100  and loss is :- 3.1041994\n",
      "Epoch:- 1120  and loss is :- 3.2006598\n",
      "Epoch:- 1140  and loss is :- 3.1739416\n",
      "Epoch:- 1160  and loss is :- 3.053585\n",
      "Epoch:- 1180  and loss is :- 3.1433845\n",
      "Epoch:- 1200  and loss is :- 3.1391437\n",
      "Epoch:- 1220  and loss is :- 2.8669481\n",
      "Epoch:- 1240  and loss is :- 2.7930756\n",
      "Epoch:- 1260  and loss is :- 2.8264177\n",
      "Epoch:- 1280  and loss is :- 2.9139924\n",
      "Epoch:- 1300  and loss is :- 2.8715136\n",
      "Epoch:- 1320  and loss is :- 2.8906693\n",
      "Epoch:- 1340  and loss is :- 2.743148\n",
      "Epoch:- 1360  and loss is :- 2.9270368\n",
      "Epoch:- 1380  and loss is :- 2.920511\n",
      "Epoch:- 1400  and loss is :- 2.90477\n",
      "Epoch:- 1420  and loss is :- 2.8429105\n",
      "Epoch:- 1440  and loss is :- 2.8564107\n",
      "Epoch:- 1460  and loss is :- 2.8320458\n",
      "Epoch:- 1480  and loss is :- 2.87295\n",
      "Epoch:- 1500  and loss is :- 2.8313847\n",
      "Epoch:- 1520  and loss is :- 2.4011528\n",
      "Epoch:- 1540  and loss is :- 2.43482\n",
      "Epoch:- 1560  and loss is :- 2.4750009\n",
      "Epoch:- 1580  and loss is :- 2.5034275\n",
      "Epoch:- 1600  and loss is :- 2.5142736\n",
      "Epoch:- 1620  and loss is :- 2.4879441\n",
      "Epoch:- 1640  and loss is :- 2.48929\n",
      "Epoch:- 1660  and loss is :- 2.567891\n",
      "Epoch:- 1680  and loss is :- 2.51168\n",
      "Epoch:- 1700  and loss is :- 2.5284216\n",
      "Epoch:- 1720  and loss is :- 2.6887014\n",
      "Epoch:- 1740  and loss is :- 2.618431\n",
      "Epoch:- 1760  and loss is :- 2.4797516\n",
      "Epoch:- 1780  and loss is :- 2.5651934\n",
      "Epoch:- 1800  and loss is :- 2.6730607\n",
      "Epoch:- 1820  and loss is :- 1.9982951\n",
      "Epoch:- 1840  and loss is :- 2.1572704\n",
      "Epoch:- 1860  and loss is :- 2.1757576\n",
      "Epoch:- 1880  and loss is :- 2.140234\n",
      "Epoch:- 1900  and loss is :- 2.3165462\n",
      "Epoch:- 1920  and loss is :- 2.237645\n",
      "Epoch:- 1940  and loss is :- 2.2885776\n",
      "Epoch:- 1960  and loss is :- 2.1984916\n",
      "Epoch:- 1980  and loss is :- 2.2423427\n",
      "Epoch:- 2000  and loss is :- 2.3587868\n",
      "Epoch:- 2020  and loss is :- 2.296575\n",
      "Epoch:- 2040  and loss is :- 2.193949\n",
      "Epoch:- 2060  and loss is :- 2.287708\n",
      "Epoch:- 2080  and loss is :- 2.3398244\n",
      "Epoch:- 2100  and loss is :- 2.2832532\n",
      "Epoch:- 2120  and loss is :- 1.8333973\n",
      "Epoch:- 2140  and loss is :- 1.8156009\n",
      "Epoch:- 2160  and loss is :- 1.9074012\n",
      "Epoch:- 2180  and loss is :- 1.8797715\n",
      "Epoch:- 2200  and loss is :- 1.9331237\n",
      "Epoch:- 2220  and loss is :- 1.9031308\n",
      "Epoch:- 2240  and loss is :- 1.9441186\n",
      "Epoch:- 2260  and loss is :- 1.9387999\n",
      "Epoch:- 2280  and loss is :- 2.078222\n",
      "Epoch:- 2300  and loss is :- 2.0645442\n",
      "Epoch:- 2320  and loss is :- 1.982272\n",
      "Epoch:- 2340  and loss is :- 1.9654543\n",
      "Epoch:- 2360  and loss is :- 2.0676596\n",
      "Epoch:- 2380  and loss is :- 1.986564\n",
      "Epoch:- 2400  and loss is :- 1.993413\n",
      "Epoch:- 2420  and loss is :- 1.5216101\n",
      "Epoch:- 2440  and loss is :- 1.5300611\n",
      "Epoch:- 2460  and loss is :- 1.5412834\n",
      "Epoch:- 2480  and loss is :- 1.708758\n",
      "Epoch:- 2500  and loss is :- 1.5574518\n",
      "Epoch:- 2520  and loss is :- 1.6042062\n",
      "Epoch:- 2540  and loss is :- 1.6914779\n",
      "Epoch:- 2560  and loss is :- 1.7286705\n",
      "Epoch:- 2580  and loss is :- 1.6533854\n",
      "Epoch:- 2600  and loss is :- 1.6583142\n",
      "Epoch:- 2620  and loss is :- 1.7254716\n",
      "Epoch:- 2640  and loss is :- 1.7608091\n",
      "Epoch:- 2660  and loss is :- 1.7993737\n",
      "Epoch:- 2680  and loss is :- 1.740849\n",
      "Epoch:- 2700  and loss is :- 1.8428551\n",
      "Epoch:- 2720  and loss is :- 1.1261911\n",
      "Epoch:- 2740  and loss is :- 1.3793875\n",
      "Epoch:- 2760  and loss is :- 1.3280529\n",
      "Epoch:- 2780  and loss is :- 1.3606883\n",
      "Epoch:- 2800  and loss is :- 1.3019255\n",
      "Epoch:- 2820  and loss is :- 1.488452\n",
      "Epoch:- 2840  and loss is :- 1.4122366\n",
      "Epoch:- 2860  and loss is :- 1.4232677\n",
      "Epoch:- 2880  and loss is :- 1.3344377\n",
      "Epoch:- 2900  and loss is :- 1.4908471\n",
      "Epoch:- 2920  and loss is :- 1.3837569\n",
      "Epoch:- 2940  and loss is :- 1.4103581\n",
      "Epoch:- 2960  and loss is :- 1.4964973\n",
      "Epoch:- 2980  and loss is :- 1.4994366\n",
      "Epoch:- 3000  and loss is :- 1.4739085\n",
      "Epoch:- 3020  and loss is :- 1.0258728\n",
      "Epoch:- 3040  and loss is :- 1.0552843\n",
      "Epoch:- 3060  and loss is :- 1.033765\n",
      "Epoch:- 3080  and loss is :- 1.0790238\n",
      "Epoch:- 3100  and loss is :- 1.1546767\n",
      "Epoch:- 3120  and loss is :- 1.069536\n",
      "Epoch:- 3140  and loss is :- 1.0979841\n",
      "Epoch:- 3160  and loss is :- 1.217513\n",
      "Epoch:- 3180  and loss is :- 1.1964682\n",
      "Epoch:- 3200  and loss is :- 1.2680506\n",
      "Epoch:- 3220  and loss is :- 1.2022395\n",
      "Epoch:- 3240  and loss is :- 1.2267246\n",
      "Epoch:- 3260  and loss is :- 1.2179677\n",
      "Epoch:- 3280  and loss is :- 1.2618581\n",
      "Epoch:- 3300  and loss is :- 1.3247117\n",
      "Epoch:- 3320  and loss is :- 0.86868364\n",
      "Epoch:- 3340  and loss is :- 0.831301\n",
      "Epoch:- 3360  and loss is :- 0.99647915\n",
      "Epoch:- 3380  and loss is :- 0.8791165\n",
      "Epoch:- 3400  and loss is :- 0.86268884\n",
      "Epoch:- 3420  and loss is :- 0.91480374\n",
      "Epoch:- 3440  and loss is :- 1.0079235\n",
      "Epoch:- 3460  and loss is :- 0.94484305\n",
      "Epoch:- 3480  and loss is :- 0.98997396\n",
      "Epoch:- 3500  and loss is :- 0.9724791\n",
      "Epoch:- 3520  and loss is :- 1.0388932\n",
      "Epoch:- 3540  and loss is :- 1.0221097\n",
      "Epoch:- 3560  and loss is :- 0.97723716\n",
      "Epoch:- 3580  and loss is :- 1.0223849\n",
      "Epoch:- 3600  and loss is :- 0.94674563\n",
      "Epoch:- 3620  and loss is :- 0.63215345\n",
      "Epoch:- 3640  and loss is :- 0.658568\n",
      "Epoch:- 3660  and loss is :- 0.6751607\n",
      "Epoch:- 3680  and loss is :- 0.6273805\n",
      "Epoch:- 3700  and loss is :- 0.7407612\n",
      "Epoch:- 3720  and loss is :- 0.6715856\n",
      "Epoch:- 3740  and loss is :- 0.85910827\n",
      "Epoch:- 3760  and loss is :- 0.7986078\n",
      "Epoch:- 3780  and loss is :- 0.80928516\n",
      "Epoch:- 3800  and loss is :- 0.84503466\n",
      "Epoch:- 3820  and loss is :- 0.72787446\n",
      "Epoch:- 3840  and loss is :- 0.79572684\n",
      "Epoch:- 3860  and loss is :- 0.8082335\n",
      "Epoch:- 3880  and loss is :- 0.9074155\n",
      "Epoch:- 3900  and loss is :- 0.82057387\n",
      "Epoch:- 3920  and loss is :- 0.4944703\n",
      "Epoch:- 3940  and loss is :- 0.5301702\n",
      "Epoch:- 3960  and loss is :- 0.47943604\n",
      "Epoch:- 3980  and loss is :- 0.59129095\n",
      "Epoch:- 4000  and loss is :- 0.54970616\n",
      "Epoch:- 4020  and loss is :- 0.63689476\n",
      "Epoch:- 4040  and loss is :- 0.6762024\n",
      "Epoch:- 4060  and loss is :- 0.6345328\n",
      "Epoch:- 4080  and loss is :- 0.6210756\n",
      "Epoch:- 4100  and loss is :- 0.593002\n",
      "Epoch:- 4120  and loss is :- 0.6269395\n",
      "Epoch:- 4140  and loss is :- 0.74558926\n",
      "Epoch:- 4160  and loss is :- 0.67859364\n",
      "Epoch:- 4180  and loss is :- 0.654934\n",
      "Epoch:- 4200  and loss is :- 0.72770685\n",
      "Epoch:- 4220  and loss is :- 0.39400312\n",
      "Epoch:- 4240  and loss is :- 0.3680601\n",
      "Epoch:- 4260  and loss is :- 0.45043278\n",
      "Epoch:- 4280  and loss is :- 0.44696885\n",
      "Epoch:- 4300  and loss is :- 0.48059294\n",
      "Epoch:- 4320  and loss is :- 0.44750258\n",
      "Epoch:- 4340  and loss is :- 0.48394892\n",
      "Epoch:- 4360  and loss is :- 0.468472\n",
      "Epoch:- 4380  and loss is :- 0.4238972\n",
      "Epoch:- 4400  and loss is :- 0.5790822\n",
      "Epoch:- 4420  and loss is :- 0.48885527\n",
      "Epoch:- 4440  and loss is :- 0.46771485\n",
      "Epoch:- 4460  and loss is :- 0.44231683\n",
      "Epoch:- 4480  and loss is :- 0.5451898\n",
      "Epoch:- 4500  and loss is :- 0.5839759\n",
      "Epoch:- 4520  and loss is :- 0.31332517\n",
      "Epoch:- 4540  and loss is :- 0.28795192\n",
      "Epoch:- 4560  and loss is :- 0.36355734\n",
      "Epoch:- 4580  and loss is :- 0.3206034\n",
      "Epoch:- 4600  and loss is :- 0.3882918\n",
      "Epoch:- 4620  and loss is :- 0.33114728\n",
      "Epoch:- 4640  and loss is :- 0.38707885\n",
      "Epoch:- 4660  and loss is :- 0.38223162\n",
      "Epoch:- 4680  and loss is :- 0.34376064\n",
      "Epoch:- 4700  and loss is :- 0.40233767\n",
      "Epoch:- 4720  and loss is :- 0.3384462\n",
      "Epoch:- 4740  and loss is :- 0.37402776\n",
      "Epoch:- 4760  and loss is :- 0.43193927\n",
      "Epoch:- 4780  and loss is :- 0.45293278\n",
      "Epoch:- 4800  and loss is :- 0.41626105\n",
      "Epoch:- 4820  and loss is :- 0.25072244\n",
      "Epoch:- 4840  and loss is :- 0.24010716\n",
      "Epoch:- 4860  and loss is :- 0.22409871\n",
      "Epoch:- 4880  and loss is :- 0.27925676\n",
      "Epoch:- 4900  and loss is :- 0.26484975\n",
      "Epoch:- 4920  and loss is :- 0.2682824\n",
      "Epoch:- 4940  and loss is :- 0.30292323\n",
      "Epoch:- 4960  and loss is :- 0.31728652\n",
      "Epoch:- 4980  and loss is :- 0.31562752\n",
      "Epoch:- 5000  and loss is :- 0.33151734\n",
      "Epoch:- 5020  and loss is :- 0.3718168\n",
      "Epoch:- 5040  and loss is :- 0.40531012\n",
      "Epoch:- 5060  and loss is :- 0.3778722\n",
      "Epoch:- 5080  and loss is :- 0.36793584\n",
      "Epoch:- 5100  and loss is :- 0.31016394\n",
      "Epoch:- 5120  and loss is :- 0.22206621\n",
      "Epoch:- 5140  and loss is :- 0.21538195\n",
      "Epoch:- 5160  and loss is :- 0.20628235\n",
      "Epoch:- 5180  and loss is :- 0.19063845\n",
      "Epoch:- 5200  and loss is :- 0.25665575\n",
      "Epoch:- 5220  and loss is :- 0.29221505\n",
      "Epoch:- 5240  and loss is :- 0.1907529\n",
      "Epoch:- 5260  and loss is :- 0.23124339\n",
      "Epoch:- 5280  and loss is :- 0.24762306\n",
      "Epoch:- 5300  and loss is :- 0.20684505\n",
      "Epoch:- 5320  and loss is :- 0.2364717\n",
      "Epoch:- 5340  and loss is :- 0.26764274\n",
      "Epoch:- 5360  and loss is :- 0.25568038\n",
      "Epoch:- 5380  and loss is :- 0.28762022\n",
      "Epoch:- 5400  and loss is :- 0.3329911\n",
      "Epoch:- 5420  and loss is :- 0.14713605\n",
      "Epoch:- 5440  and loss is :- 0.14569895\n",
      "Epoch:- 5460  and loss is :- 0.15947407\n",
      "Epoch:- 5480  and loss is :- 0.19937745\n",
      "Epoch:- 5500  and loss is :- 0.1671312\n",
      "Epoch:- 5520  and loss is :- 0.15570943\n",
      "Epoch:- 5540  and loss is :- 0.16788569\n",
      "Epoch:- 5560  and loss is :- 0.20270821\n",
      "Epoch:- 5580  and loss is :- 0.20173348\n",
      "Epoch:- 5600  and loss is :- 0.19026011\n",
      "Epoch:- 5620  and loss is :- 0.21160233\n",
      "Epoch:- 5640  and loss is :- 0.22213173\n",
      "Epoch:- 5660  and loss is :- 0.26832452\n",
      "Epoch:- 5680  and loss is :- 0.22054689\n",
      "Epoch:- 5700  and loss is :- 0.20332354\n",
      "Epoch:- 5720  and loss is :- 0.14596072\n",
      "Epoch:- 5740  and loss is :- 0.12351363\n",
      "Epoch:- 5760  and loss is :- 0.13509971\n",
      "Epoch:- 5780  and loss is :- 0.14899726\n",
      "Epoch:- 5800  and loss is :- 0.14587861\n",
      "Epoch:- 5820  and loss is :- 0.16819687\n",
      "Epoch:- 5840  and loss is :- 0.15710911\n",
      "Epoch:- 5860  and loss is :- 0.17065865\n",
      "Epoch:- 5880  and loss is :- 0.13119873\n",
      "Epoch:- 5900  and loss is :- 0.20083962\n",
      "Epoch:- 5920  and loss is :- 0.21098962\n",
      "Epoch:- 5940  and loss is :- 0.15274957\n",
      "Epoch:- 5960  and loss is :- 0.20725755\n",
      "Epoch:- 5980  and loss is :- 0.213707\n",
      "Epoch:- 6000  and loss is :- 0.20520091\n",
      "Epoch:- 6020  and loss is :- 0.11562725\n",
      "Epoch:- 6040  and loss is :- 0.11053262\n",
      "Epoch:- 6060  and loss is :- 0.10654823\n",
      "Epoch:- 6080  and loss is :- 0.11930236\n",
      "Epoch:- 6100  and loss is :- 0.11255625\n",
      "Epoch:- 6120  and loss is :- 0.11090431\n",
      "Epoch:- 6140  and loss is :- 0.117785834\n",
      "Epoch:- 6160  and loss is :- 0.13073416\n",
      "Epoch:- 6180  and loss is :- 0.15306063\n",
      "Epoch:- 6200  and loss is :- 0.10706631\n",
      "Epoch:- 6220  and loss is :- 0.15132658\n",
      "Epoch:- 6240  and loss is :- 0.19084974\n",
      "Epoch:- 6260  and loss is :- 0.16791597\n",
      "Epoch:- 6280  and loss is :- 0.1507579\n",
      "Epoch:- 6300  and loss is :- 0.17792144\n",
      "Epoch:- 6320  and loss is :- 0.08876179\n",
      "Epoch:- 6340  and loss is :- 0.08403343\n",
      "Epoch:- 6360  and loss is :- 0.08526605\n",
      "Epoch:- 6380  and loss is :- 0.090277016\n",
      "Epoch:- 6400  and loss is :- 0.11008032\n",
      "Epoch:- 6420  and loss is :- 0.0938519\n",
      "Epoch:- 6440  and loss is :- 0.11219679\n",
      "Epoch:- 6460  and loss is :- 0.10047633\n",
      "Epoch:- 6480  and loss is :- 0.11799118\n",
      "Epoch:- 6500  and loss is :- 0.10270459\n",
      "Epoch:- 6520  and loss is :- 0.11524153\n",
      "Epoch:- 6540  and loss is :- 0.12222158\n",
      "Epoch:- 6560  and loss is :- 0.13211253\n",
      "Epoch:- 6580  and loss is :- 0.16478562\n",
      "Epoch:- 6600  and loss is :- 0.16061312\n",
      "Epoch:- 6620  and loss is :- 0.0849695\n",
      "Epoch:- 6640  and loss is :- 0.087645195\n",
      "Epoch:- 6660  and loss is :- 0.07020035\n",
      "Epoch:- 6680  and loss is :- 0.08565467\n",
      "Epoch:- 6700  and loss is :- 0.1032422\n",
      "Epoch:- 6720  and loss is :- 0.11856119\n",
      "Epoch:- 6740  and loss is :- 0.096451625\n",
      "Epoch:- 6760  and loss is :- 0.08964265\n",
      "Epoch:- 6780  and loss is :- 0.09562891\n",
      "Epoch:- 6800  and loss is :- 0.095695145\n",
      "Epoch:- 6820  and loss is :- 0.106869794\n",
      "Epoch:- 6840  and loss is :- 0.09577536\n",
      "Epoch:- 6860  and loss is :- 0.13596343\n",
      "Epoch:- 6880  and loss is :- 0.11340424\n",
      "Epoch:- 6900  and loss is :- 0.11775932\n",
      "Epoch:- 6920  and loss is :- 0.07067776\n",
      "Epoch:- 6940  and loss is :- 0.07495469\n",
      "Epoch:- 6960  and loss is :- 0.083106115\n",
      "Epoch:- 6980  and loss is :- 0.07184742\n",
      "Epoch:- 7000  and loss is :- 0.0474558\n",
      "Epoch:- 7020  and loss is :- 0.0707209\n",
      "Epoch:- 7040  and loss is :- 0.072686106\n",
      "Epoch:- 7060  and loss is :- 0.08786449\n",
      "Epoch:- 7080  and loss is :- 0.09406202\n",
      "Epoch:- 7100  and loss is :- 0.08839196\n",
      "Epoch:- 7120  and loss is :- 0.125179\n",
      "Epoch:- 7140  and loss is :- 0.11526284\n",
      "Epoch:- 7160  and loss is :- 0.084819406\n",
      "Epoch:- 7180  and loss is :- 0.09352331\n",
      "Epoch:- 7200  and loss is :- 0.1048153\n",
      "Epoch:- 7220  and loss is :- 0.06728978\n",
      "Epoch:- 7240  and loss is :- 0.054941215\n",
      "Epoch:- 7260  and loss is :- 0.069209464\n",
      "Epoch:- 7280  and loss is :- 0.05854798\n",
      "Epoch:- 7300  and loss is :- 0.06880706\n",
      "Epoch:- 7320  and loss is :- 0.06616038\n",
      "Epoch:- 7340  and loss is :- 0.07850749\n",
      "Epoch:- 7360  and loss is :- 0.0801004\n",
      "Epoch:- 7380  and loss is :- 0.07788383\n",
      "Epoch:- 7400  and loss is :- 0.075588286\n",
      "Epoch:- 7420  and loss is :- 0.08368278\n",
      "Epoch:- 7440  and loss is :- 0.06433589\n",
      "Epoch:- 7460  and loss is :- 0.081248626\n",
      "Epoch:- 7480  and loss is :- 0.078490295\n",
      "Epoch:- 7500  and loss is :- 0.07169155\n",
      "Epoch:- 7520  and loss is :- 0.051072676\n",
      "Epoch:- 7540  and loss is :- 0.04412158\n",
      "Epoch:- 7560  and loss is :- 0.07536593\n",
      "Epoch:- 7580  and loss is :- 0.048303775\n",
      "Epoch:- 7600  and loss is :- 0.04650702\n",
      "Epoch:- 7620  and loss is :- 0.069158494\n",
      "Epoch:- 7640  and loss is :- 0.053590406\n",
      "Epoch:- 7660  and loss is :- 0.06071175\n",
      "Epoch:- 7680  and loss is :- 0.04549213\n",
      "Epoch:- 7700  and loss is :- 0.06372976\n",
      "Epoch:- 7720  and loss is :- 0.076943636\n",
      "Epoch:- 7740  and loss is :- 0.07449242\n",
      "Epoch:- 7760  and loss is :- 0.06921084\n",
      "Epoch:- 7780  and loss is :- 0.06983618\n",
      "Epoch:- 7800  and loss is :- 0.06977658\n",
      "Epoch:- 7820  and loss is :- 0.048367526\n",
      "Epoch:- 7840  and loss is :- 0.054676756\n",
      "Epoch:- 7860  and loss is :- 0.044406116\n",
      "Epoch:- 7880  and loss is :- 0.043704715\n",
      "Epoch:- 7900  and loss is :- 0.045356967\n",
      "Epoch:- 7920  and loss is :- 0.053012542\n",
      "Epoch:- 7940  and loss is :- 0.03575229\n",
      "Epoch:- 7960  and loss is :- 0.041601077\n",
      "Epoch:- 7980  and loss is :- 0.061432477\n",
      "Epoch:- 8000  and loss is :- 0.061842717\n",
      "Epoch:- 8020  and loss is :- 0.056729566\n",
      "Epoch:- 8040  and loss is :- 0.067929074\n",
      "Epoch:- 8060  and loss is :- 0.060913883\n",
      "Epoch:- 8080  and loss is :- 0.065590054\n",
      "Epoch:- 8100  and loss is :- 0.08138139\n",
      "Epoch:- 8120  and loss is :- 0.0456373\n",
      "Epoch:- 8140  and loss is :- 0.08098092\n",
      "Epoch:- 8160  and loss is :- 0.0495957\n",
      "Epoch:- 8180  and loss is :- 0.052512765\n",
      "Epoch:- 8200  and loss is :- 0.049077127\n",
      "Epoch:- 8220  and loss is :- 0.04466638\n",
      "Epoch:- 8240  and loss is :- 0.050562892\n",
      "Epoch:- 8260  and loss is :- 0.04083626\n",
      "Epoch:- 8280  and loss is :- 0.06385726\n",
      "Epoch:- 8300  and loss is :- 0.06652221\n",
      "Epoch:- 8320  and loss is :- 0.05171412\n",
      "Epoch:- 8340  and loss is :- 0.05691699\n",
      "Epoch:- 8360  and loss is :- 0.06208148\n",
      "Epoch:- 8380  and loss is :- 0.046601225\n",
      "Epoch:- 8400  and loss is :- 0.05401162\n",
      "Epoch:- 8420  and loss is :- 0.0351077\n",
      "Epoch:- 8440  and loss is :- 0.04118426\n",
      "Epoch:- 8460  and loss is :- 0.0341823\n",
      "Epoch:- 8480  and loss is :- 0.03365035\n",
      "Epoch:- 8500  and loss is :- 0.033513006\n",
      "Epoch:- 8520  and loss is :- 0.041121222\n",
      "Epoch:- 8540  and loss is :- 0.04724882\n",
      "Epoch:- 8560  and loss is :- 0.04585013\n",
      "Epoch:- 8580  and loss is :- 0.04802652\n",
      "Epoch:- 8600  and loss is :- 0.04273758\n",
      "Epoch:- 8620  and loss is :- 0.04627738\n",
      "Epoch:- 8640  and loss is :- 0.056037612\n",
      "Epoch:- 8660  and loss is :- 0.059687644\n",
      "Epoch:- 8680  and loss is :- 0.060461894\n",
      "Epoch:- 8700  and loss is :- 0.055811282\n",
      "Epoch:- 8720  and loss is :- 0.034462295\n",
      "Epoch:- 8740  and loss is :- 0.029131508\n",
      "Epoch:- 8760  and loss is :- 0.029478677\n",
      "Epoch:- 8780  and loss is :- 0.035844557\n",
      "Epoch:- 8800  and loss is :- 0.041616835\n",
      "Epoch:- 8820  and loss is :- 0.040254653\n",
      "Epoch:- 8840  and loss is :- 0.041231308\n",
      "Epoch:- 8860  and loss is :- 0.046816755\n",
      "Epoch:- 8880  and loss is :- 0.039592136\n",
      "Epoch:- 8900  and loss is :- 0.04356949\n",
      "Epoch:- 8920  and loss is :- 0.0447\n",
      "Epoch:- 8940  and loss is :- 0.05419526\n",
      "Epoch:- 8960  and loss is :- 0.05346563\n",
      "Epoch:- 8980  and loss is :- 0.051848188\n",
      "Epoch:- 9000  and loss is :- 0.05234064\n",
      "Epoch:- 9020  and loss is :- 0.0372445\n",
      "Epoch:- 9040  and loss is :- 0.03124552\n",
      "Epoch:- 9060  and loss is :- 0.028175259\n",
      "Epoch:- 9080  and loss is :- 0.05043426\n",
      "Epoch:- 9100  and loss is :- 0.049867123\n",
      "Epoch:- 9120  and loss is :- 0.03236123\n",
      "Epoch:- 9140  and loss is :- 0.04879734\n",
      "Epoch:- 9160  and loss is :- 0.036643017\n",
      "Epoch:- 9180  and loss is :- 0.049045153\n",
      "Epoch:- 9200  and loss is :- 0.038683165\n",
      "Epoch:- 9220  and loss is :- 0.044438496\n",
      "Epoch:- 9240  and loss is :- 0.045930337\n",
      "Epoch:- 9260  and loss is :- 0.0424128\n",
      "Epoch:- 9280  and loss is :- 0.051485896\n",
      "Epoch:- 9300  and loss is :- 0.066035174\n",
      "Epoch:- 9320  and loss is :- 0.035663743\n",
      "Epoch:- 9340  and loss is :- 0.03565176\n",
      "Epoch:- 9360  and loss is :- 0.034272518\n",
      "Epoch:- 9380  and loss is :- 0.0363381\n",
      "Epoch:- 9400  and loss is :- 0.01924722\n",
      "Epoch:- 9420  and loss is :- 0.030036947\n",
      "Epoch:- 9440  and loss is :- 0.027149487\n",
      "Epoch:- 9460  and loss is :- 0.04026011\n",
      "Epoch:- 9480  and loss is :- 0.03837322\n",
      "Epoch:- 9500  and loss is :- 0.035069738\n",
      "Epoch:- 9520  and loss is :- 0.04485214\n",
      "Epoch:- 9540  and loss is :- 0.03279309\n",
      "Epoch:- 9560  and loss is :- 0.05548672\n",
      "Epoch:- 9580  and loss is :- 0.054501865\n",
      "Epoch:- 9600  and loss is :- 0.05632096\n",
      "Epoch:- 9620  and loss is :- 0.024239793\n",
      "Epoch:- 9640  and loss is :- 0.023909489\n",
      "Epoch:- 9660  and loss is :- 0.045371883\n",
      "Epoch:- 9680  and loss is :- 0.025936238\n",
      "Epoch:- 9700  and loss is :- 0.044038158\n",
      "Epoch:- 9720  and loss is :- 0.041951608\n",
      "Epoch:- 9740  and loss is :- 0.02996144\n",
      "Epoch:- 9760  and loss is :- 0.040452853\n",
      "Epoch:- 9780  and loss is :- 0.039674647\n",
      "Epoch:- 9800  and loss is :- 0.033581108\n",
      "Epoch:- 9820  and loss is :- 0.0410431\n",
      "Epoch:- 9840  and loss is :- 0.037764333\n",
      "Epoch:- 9860  and loss is :- 0.05073473\n",
      "Epoch:- 9880  and loss is :- 0.040285688\n",
      "Epoch:- 9900  and loss is :- 0.03308332\n",
      "Epoch:- 9920  and loss is :- 0.025765108\n",
      "Epoch:- 9940  and loss is :- 0.02408774\n",
      "Epoch:- 9960  and loss is :- 0.035682764\n",
      "Epoch:- 9980  and loss is :- 0.03414433\n",
      "Epoch:- 10000  and loss is :- 0.0304613\n",
      "Epoch:- 10020  and loss is :- 0.027973522\n",
      "Epoch:- 10040  and loss is :- 0.044175245\n",
      "Epoch:- 10060  and loss is :- 0.041482195\n",
      "Epoch:- 10080  and loss is :- 0.028810503\n",
      "Epoch:- 10100  and loss is :- 0.03917243\n",
      "Epoch:- 10120  and loss is :- 0.03716014\n",
      "Epoch:- 10140  and loss is :- 0.045625124\n",
      "Epoch:- 10160  and loss is :- 0.026810851\n",
      "Epoch:- 10180  and loss is :- 0.04665458\n",
      "Epoch:- 10200  and loss is :- 0.0271419\n",
      "Epoch:- 10220  and loss is :- 0.028140815\n",
      "Epoch:- 10240  and loss is :- 0.018519282\n",
      "Epoch:- 10260  and loss is :- 0.024409186\n",
      "Epoch:- 10280  and loss is :- 0.026496595\n",
      "Epoch:- 10300  and loss is :- 0.036416907\n",
      "Epoch:- 10320  and loss is :- 0.027949052\n",
      "Epoch:- 10340  and loss is :- 0.024628555\n",
      "Epoch:- 10360  and loss is :- 0.03502419\n",
      "Epoch:- 10380  and loss is :- 0.022946086\n",
      "Epoch:- 10400  and loss is :- 0.023445794\n",
      "Epoch:- 10420  and loss is :- 0.031001072\n",
      "Epoch:- 10440  and loss is :- 0.0346462\n",
      "Epoch:- 10460  and loss is :- 0.046725277\n",
      "Epoch:- 10480  and loss is :- 0.056229006\n",
      "Epoch:- 10500  and loss is :- 0.034547973\n",
      "Epoch:- 10520  and loss is :- 0.0242834\n",
      "Epoch:- 10540  and loss is :- 0.030608239\n",
      "Epoch:- 10560  and loss is :- 0.027206063\n",
      "Epoch:- 10580  and loss is :- 0.019476406\n",
      "Epoch:- 10600  and loss is :- 0.028922161\n",
      "Epoch:- 10620  and loss is :- 0.022736184\n",
      "Epoch:- 10640  and loss is :- 0.028264377\n",
      "Epoch:- 10660  and loss is :- 0.030685026\n",
      "Epoch:- 10680  and loss is :- 0.03182821\n",
      "Epoch:- 10700  and loss is :- 0.029187936\n",
      "Epoch:- 10720  and loss is :- 0.03895646\n",
      "Epoch:- 10740  and loss is :- 0.029645856\n",
      "Epoch:- 10760  and loss is :- 0.034996778\n",
      "Epoch:- 10780  and loss is :- 0.028526835\n",
      "Epoch:- 10800  and loss is :- 0.053024482\n",
      "Epoch:- 10820  and loss is :- 0.023805993\n",
      "Epoch:- 10840  and loss is :- 0.023052778\n",
      "Epoch:- 10860  and loss is :- 0.026701927\n",
      "Epoch:- 10880  and loss is :- 0.018489609\n",
      "Epoch:- 10900  and loss is :- 0.03969307\n",
      "Epoch:- 10920  and loss is :- 0.01600614\n",
      "Epoch:- 10940  and loss is :- 0.023180183\n",
      "Epoch:- 10960  and loss is :- 0.018491354\n",
      "Epoch:- 10980  and loss is :- 0.023289654\n",
      "Epoch:- 11000  and loss is :- 0.025946883\n",
      "Epoch:- 11020  and loss is :- 0.03135843\n",
      "Epoch:- 11040  and loss is :- 0.02378493\n",
      "Epoch:- 11060  and loss is :- 0.046969254\n",
      "Epoch:- 11080  and loss is :- 0.030179543\n",
      "Epoch:- 11100  and loss is :- 0.028301934\n",
      "Epoch:- 11120  and loss is :- 0.020828882\n",
      "Epoch:- 11140  and loss is :- 0.038665883\n",
      "Epoch:- 11160  and loss is :- 0.022637755\n",
      "Epoch:- 11180  and loss is :- 0.024638582\n",
      "Epoch:- 11200  and loss is :- 0.024414062\n",
      "Epoch:- 11220  and loss is :- 0.026249439\n",
      "Epoch:- 11240  and loss is :- 0.033120632\n",
      "Epoch:- 11260  and loss is :- 0.026130589\n",
      "Epoch:- 11280  and loss is :- 0.02725614\n",
      "Epoch:- 11300  and loss is :- 0.03453569\n",
      "Epoch:- 11320  and loss is :- 0.035263143\n",
      "Epoch:- 11340  and loss is :- 0.03666389\n",
      "Epoch:- 11360  and loss is :- 0.024776174\n",
      "Epoch:- 11380  and loss is :- 0.037766177\n",
      "Epoch:- 11400  and loss is :- 0.03900403\n",
      "Epoch:- 11420  and loss is :- 0.02930081\n",
      "Epoch:- 11440  and loss is :- 0.01879489\n",
      "Epoch:- 11460  and loss is :- 0.032906953\n",
      "Epoch:- 11480  and loss is :- 0.020169651\n",
      "Epoch:- 11500  and loss is :- 0.027971158\n",
      "Epoch:- 11520  and loss is :- 0.025961367\n",
      "Epoch:- 11540  and loss is :- 0.028936142\n",
      "Epoch:- 11560  and loss is :- 0.020144587\n",
      "Epoch:- 11580  and loss is :- 0.018780963\n",
      "Epoch:- 11600  and loss is :- 0.028641665\n",
      "Epoch:- 11620  and loss is :- 0.025547234\n",
      "Epoch:- 11640  and loss is :- 0.04089562\n",
      "Epoch:- 11660  and loss is :- 0.03522519\n",
      "Epoch:- 11680  and loss is :- 0.03556926\n",
      "Epoch:- 11700  and loss is :- 0.043302752\n",
      "Epoch:- 11720  and loss is :- 0.03033376\n",
      "Epoch:- 11740  and loss is :- 0.02891737\n",
      "Epoch:- 11760  and loss is :- 0.022805309\n",
      "Epoch:- 11780  and loss is :- 0.027496018\n",
      "Epoch:- 11800  and loss is :- 0.02796619\n",
      "Epoch:- 11820  and loss is :- 0.022469172\n",
      "Epoch:- 11840  and loss is :- 0.020454958\n",
      "Epoch:- 11860  and loss is :- 0.0334001\n",
      "Epoch:- 11880  and loss is :- 0.022092756\n",
      "Epoch:- 11900  and loss is :- 0.026269186\n",
      "Epoch:- 11920  and loss is :- 0.023320505\n",
      "Epoch:- 11940  and loss is :- 0.025293788\n",
      "Epoch:- 11960  and loss is :- 0.021800853\n",
      "Epoch:- 11980  and loss is :- 0.027148547\n",
      "Epoch:- 12000  and loss is :- 0.029758403\n"
     ]
    }
   ],
   "source": [
    "for i in range(40):\n",
    "    index=np.arange(30000)\n",
    "    np.random.shuffle(index)\n",
    "    for j in range(0,30000,100):\n",
    "        ind=index[j:j+100]\n",
    "        \n",
    "        xinp=texttokenpad[ind]\n",
    "        yinp1=headlineinpad[ind]\n",
    "        yout1=headlineoutpad[ind]\n",
    "    \n",
    "        #print(xinp.shape)\n",
    "        #print(yinp1.shape)\n",
    "        #print(yout1.shape)\n",
    "        _=sess.run(trainstep,feed_dict={x:xinp,yin:yinp1,yout:yout1})\n",
    "        loss2=loss.eval({x:xinp,yin:yinp1,yout:yout1})\n",
    "        count+=1\n",
    "        trainloss.append(loss2)\n",
    "        if count%20==0:\n",
    "            print(\"Epoch:-\",count,\" and loss is :-\",loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faa70094710>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trainloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(enout1,dinp1):\n",
    "    \n",
    "    deinput=tf.constant(np.array(dinp1).reshape((1,-1)),dtype=tf.int64)\n",
    "    #print(deinput.shape)\n",
    "    prediction1=decoder2.call(enout1,deinput)\n",
    "    #print(prediction1.get_shape())\n",
    "    prediction=tf.nn.softmax(prediction1,axis=-1)\n",
    "    firstval=prediction.eval()\n",
    "    #print(firstval.shape)\n",
    "    word1=headindword[np.argmax(firstval[0,-1,:])]\n",
    "    \n",
    "    return word1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "headindword=dict([(j,i) for i,j in headwordind.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "textindword=dict([(j,i) for i,j in textwordind.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16503"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(headindword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def printsent(index):\n",
    "    enout=encoderoutput.eval({x:texttokenpad[index].reshape(1,texttokenpad.shape[1])})\n",
    "    enout1=tf.constant(enout)\n",
    "    count=0\n",
    "    inp=[headwordind['<start>']]\n",
    "    word='<start>'\n",
    "    li=[]\n",
    "    while count<15:\n",
    "        #print(word)\n",
    "        #print(inp)\n",
    "        word=predict(enout1,inp)\n",
    "        li.append(word)\n",
    "        inp.append(headwordind[word])\n",
    "        count+=1\n",
    "    del(enout1)\n",
    "    \n",
    "    print(\"predicted sentence:\",\" \".join(li))\n",
    "    print(\"----\")\n",
    "    print(\"full text:-\",\" \".join([textindword[i] for i in texttokenpad[index]]))\n",
    "    print(\"----\")\n",
    "    print(\"original summary:-\",\" \".join([headindword[i] for i in headlineoutpad[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted sentence: no one would spend num num cr to change his image dutt on sanju <end>\n",
      "----\n",
      "full text:- dismissing allegations against director rajkumar hirani of whitewashing his image through sanju sanjay dutt on thursday said don think anyone would spend num num crore to change his image he added ve told the truth and it has been accepted by india the movie collection shows that after the film ended broke down couldn believe it he further said <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----\n",
      "original summary:- no one would spend num num cr to change his image dutt on sanju <end> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "printsent(3) # After training for 18000 steps and 60 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted sentence: force india claim numth and numth place in spanish grand prix <end> <end> <end> <end>\n",
      "----\n",
      "full text:- force india formula one team drivers sergio perez and <unk> <unk> finished in the fourth and fifth spot respectively in the spanish grand prix in barcelona on sunday mercedes lewis hamilton won the race by beating ferrari sebastian vettel and red bull daniel ricciardo who finished second and third respectively <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----\n",
      "original summary:- force india claim numth and numth place in spanish grand prix <end> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "printsent(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted sentence: indian startup rooter among num chosen for sports accelerator program <end> <end> <end> <end> <end>\n",
      "----\n",
      "full text:- sports social engagement platform rooter is the only indian startup among the num startups chosen for num lead sports accelerator program founded by piyush kumar and akshat goel in num rooter is sports social gaming platform that connects sports fans and engages them during live matches it also offers them match prediction games sports social feed across eight sports <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----\n",
      "original summary:- indian startup rooter among num chosen for sports accelerator program <end> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "printsent(6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
