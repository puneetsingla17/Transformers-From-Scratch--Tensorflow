{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport nltk\nimport re\nimport os\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport glob\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\nUsing TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/news-summary/news_summary_more.csv\")","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"                                           headlines  \\\n0  upGrad learner switches to career in ML & Al w...   \n1  Delhi techie wins free food from Swiggy for on...   \n2  New Zealand end Rohit Sharma-led India's 12-ma...   \n3  Aegon life iTerm insurance plan helps customer...   \n4  Have known Hirani for yrs, what if MeToo claim...   \n\n                                                text  \n0  Saurav Kant, an alumnus of upGrad and IIIT-B's...  \n1  Kunal Shah's credit card bill payment platform...  \n2  New Zealand defeated India by 8 wickets in the...  \n3  With Aegon Life iTerm Insurance plan, customer...  \n4  Speaking about the sexual harassment allegatio...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>headlines</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Delhi techie wins free food from Swiggy for on...</td>\n      <td>Kunal Shah's credit card bill payment platform...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n      <td>New Zealand defeated India by 8 wickets in the...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Aegon life iTerm insurance plan helps customer...</td>\n      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n      <td>Speaking about the sexual harassment allegatio...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=df.sample(30000)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text=df1.text","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlines=df1.headlines","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text.head()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"17376    A 48-year-old vegetable vendor named Dadarao B...\n21627    The Turkish lira has become more volatile than...\n42000    The Indian Railways on Monday revealed that it...\n4345     Following a phone call with Nelson Mandela in ...\n91536    Former India hockey captain and Rajya Sabha MP...\nName: text, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(text):\n    text=text.lower()\n    text=text.strip()\n    #text=text.replace(\"'s\",\"\",text)\n    text=re.sub(r'[^\\w\\d]',\" \",text)\n    text=re.sub(r'\\d+','num',text)\n    text=re.sub(r\"ยน\",\"\",text)\n    text=re.sub(r\"\\s\\w\\s\",\" \",text)\n    text=re.sub(r\"\\s{2,}\",\" \",text)\n    text=text.strip()\n    return text","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess(text.iloc[3])","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"'following phone call with nelson mandela in june num former uk pm margaret thatcher had told her then foreign affairs adviser that the ex south african president had closed mind according to newly released secret files the uk then ambassador to south africa sir robin renwick had said mandela wasn as intelligent as ex zimbabwe president robert mugabe the files further revealed'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"processtext=text.apply(lambda x:preprocess(x))","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processheadline=headlines.apply(lambda x:preprocess(x))","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processheadline.iloc[0]","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"'man fills num potholes in mumbai in num yrs after son death'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"processtext.iloc[0]","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"'a num year old vegetable vendor named dadarao bilhore has filled in almost num potholes across mumbai over the past three years after his son died in road accident bilhore num year old son was travelling pillion on motorbike which hit pothole in july num also don want anyone else to lose loved one like we have bilhore said'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"textwordcount=Counter()\nfor i in range(len(processtext)):\n    textwordcount.update(processtext.iloc[i].split())","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlinewordcount=Counter()\nfor i in range(len(processheadline)):\n    headlinewordcount.update(processheadline.iloc[i].split())","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len([x for x,i in textwordcount.items() if i>2])","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"23293"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len([x for x,i in headlinewordcount.items() if i>1])","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"12816"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"textokenizer=Tokenizer(num_words=24000,oov_token='<unk>')\n","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"textokenizer.fit_on_texts(processtext)\ntexttoken=textokenizer.texts_to_sequences(processtext)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"textwordind=textokenizer.word_index\ntextwordind=dict([(i,j) for i,j in textwordind.items() if j<=25000])","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"textwordind['<pad>']=0","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headtokenizer=Tokenizer(num_words=12500,oov_token='<unk>')\nheadtokenizer.fit_on_texts(processheadline)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headwordind=headtokenizer.word_index\nheadwordind=dict([(i,j) for i,j in headwordind.items() if j<=13500])","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headwordind['<pad>']=0","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headwordind['<start>']=len(headwordind)\nheadwordind['<end>']=len(headwordind)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlinetoken=headtokenizer.texts_to_sequences(processheadline)","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlinetokenin=[[headwordind['<start>']]+i for i in headlinetoken]","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlinetokenout=[i+[headwordind['<end>']] for i in headlinetoken]","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlinetokenin[:4]","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"[[13501, 18, 8002, 2, 4907, 4, 62, 4, 2, 125, 10, 117, 88],\n [13501, 2849, 8003, 214, 131, 1, 126, 480],\n [13501, 570, 71, 13, 2, 78, 4499, 5, 2, 63, 547, 1],\n [13501, 50, 91, 22, 9742, 1236, 8004, 4908, 279, 2479, 2711, 6071]]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlinetokenout[:4]","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"[[18, 8002, 2, 4907, 4, 62, 4, 2, 125, 10, 117, 88, 13502],\n [2849, 8003, 214, 131, 1, 126, 480, 13502],\n [570, 71, 13, 2, 78, 4499, 5, 2, 63, 547, 1, 13502],\n [50, 91, 22, 9742, 1236, 8004, 4908, 279, 2479, 2711, 6071, 13502]]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"texttokenpad=pad_sequences(texttoken,padding='post')","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlineinpad=pad_sequences(headlinetokenin,padding='post')\nheadlineoutpad=pad_sequences(headlinetokenout,padding='post')","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texttokenpad.shape","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"(30000, 74)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlineinpad.shape","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"(30000, 17)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=tf.placeholder(shape=[None,texttokenpad.shape[1]],dtype=tf.int32)\nyinp=tf.placeholder(shape=[None,headlineinpad.shape[1]],dtype=tf.int32)\nyout=tf.placeholder(shape=[None,headlineoutpad.shape[1]],dtype=tf.int32)","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxtextlen=texttokenpad.shape[1]\nmaxsumlen=headlineinpad.shape[1]","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxtextlen","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"74"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def positional_embedding(pos, model_size):\n    PE = np.zeros((1, model_size))\n    for i in range(model_size):\n        if i % 2 == 0:\n            PE[:,i] = np.sin(pos / 10000 ** (i / model_size))\n        else:\n            PE[:,i] = np.cos(pos / 10000 ** ((i - 1) / model_size))\n    return PE\n\n# max_length = max(len(data_en[0]), len(data_fr_in[0]))\nMODEL_SIZE = 128\n\npes = []\nfor i in range(maxtextlen):\n    pes.append(positional_embedding(i, MODEL_SIZE))\n\npes = np.concatenate(pes, axis=0)\npes = tf.constant(pes, dtype=tf.float32)","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelsize=128\nh=2\nnumlayers=3\n","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def multiheadattention(modelsize,h):\n    querysize=modelsize//h  # querysize and valuesize are to be same \n    valuesize=modelsize//h\n    keysize=modelsize//h\n    wq=[tf.keras.layers.Dense(querysize,activation='relu') for _ in range(h)]\n    wk=[tf.keras.layers.Dense(querysize,activation='relu') for _ in range(h)]\n    wv=[tf.keras.layers.Dense(querysize,activation='relu') for _ in range(h)]\n    wo=tf.keras.layers.Dense(modelsize)\n    return wq,wk,wv,wo,keysize\n\n","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def call(query,value,keysize,h,wq,wk,wv,wo):\n    # query len and keylen can be different size like in decoder but in encoder they are of same length\n    # they are same only in encoder but for decoder they will be different\n    # keylen is same as value len in decoder\n    heads=[]\n    for i in range(h):\n        score=tf.matmul(wq[i](query),wk[i](value),transpose_b=True)\n        score/=tf.math.sqrt(tf.dtypes.cast(keysize,tf.float32)) # batch x querylen x keylen\n        alignment=tf.nn.softmax(score,axis=2)\n        head=tf.matmul(alignment,wv[i](value))\n        heads.append(head)\n    \n    heads=tf.concat(heads,axis=2) # adds up valuesize dims which here is 64 across 2 dim to get 128\n    # batch x valuelen x 128\n    heads=wo(heads)\n    return heads\n    ","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedvar=tf.Variable(tf.random_normal(shape=[len(textwordind),modelsize]))\n","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wq,wk,wv,wo,keysize=multiheadattention(modelsize,h)","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedvar","execution_count":44,"outputs":[{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"<tf.Variable 'Variable:0' shape=(25001, 128) dtype=float32_ref>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pes","execution_count":45,"outputs":[{"output_type":"execute_result","execution_count":45,"data":{"text/plain":"<tf.Tensor 'Const:0' shape=(74, 128) dtype=float32>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub1=[]\nfor i in range(x.shape[1]):\n    embed=tf.nn.embedding_lookup(embedvar,tf.expand_dims(x[:,i],axis=1))\n    #print(embed.get_shape())\n    sub1.append(embed+pes[i,:])\n\nsubconcat1=tf.concat(sub1,axis=1)","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since we have to add positional embeddings to embed variable thats why we are partioning x variable along sequence length\n# and then adding positional encoding to embeddings","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subconcat1","execution_count":48,"outputs":[{"output_type":"execute_result","execution_count":48,"data":{"text/plain":"<tf.Tensor 'concat:0' shape=(?, 74, 128) dtype=float32>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"subout=[]\nfor j in range(subconcat1.shape[1]):\n    attention1=call(tf.expand_dims(subconcat1[:,j,:],axis=1),subconcat1,keysize,h,wq,wk,wv,wo)\n    subout.append(attention1)\n","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subout=tf.concat(subout,axis=1)","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#One thing we can try may be here call(subconcat1,subconcat1.keysize,h,wq,wk,wv,wo)\n","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subout=subout+subconcat1","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subout=tf.keras.layers.BatchNormalization()(subout)\nffin=tf.keras.layers.Dense(4*modelsize,activation='relu')(subout)\nffin=tf.keras.layers.Dense(modelsize)(ffin)","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ffout=ffin+subout\nffoutnorm=tf.keras.layers.BatchNormalization()(ffout)","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wq2,wk2,wv2,wo2,keysize2=multiheadattention(modelsize,h)","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subin2=ffoutnorm\nsubout2=[]\nfor j in range(subconcat1.shape[1]):\n    attention2=call( tf.expand_dims(subin2[:,j,:],axis=1),subin2,keysize2,h,wq2,wk2,wv2,wo2)\n    subout2.append(attention2)\nsubout2=tf.concat(subout2,axis=1)\n    ","execution_count":56,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subout2=subout2+subin2\nsubout2=tf.keras.layers.BatchNormalization()(subout2)\nffin2 = tf.keras.layers.Dense(4*modelsize,activation=tf.nn.relu)(subout2)\nffin2 = tf.keras.layers.Dense(modelsize)(ffin2)","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ffout2=ffin2+subout2\nffoutnorm2=tf.keras.layers.BatchNormalization()(ffout2)","execution_count":58,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoderoutput=ffoutnorm2","execution_count":59,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoderoutput","execution_count":60,"outputs":[{"output_type":"execute_result","execution_count":60,"data":{"text/plain":"<tf.Tensor 'batch_normalization_3/batchnorm/add_1:0' shape=(?, 74, 128) dtype=float32>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class decoder:\n    def __init__(self,modelsize,h):\n        \n        self.modelsize=modelsize\n        self.h=h\n        self.dembedvar=tf.Variable(tf.random_normal(shape=(len(headwordind),self.modelsize)))\n        self.dwq1,self.dwk1,self.dwv1,self.dwo1,self.dkeysize1=multiheadattention(self.modelsize,self.h)\n        self.dwq2,self.dwk2,self.dwv2,self.dwo2,self.dkeysize2=multiheadattention(self.modelsize,self.h)\n        self.damwq1,self.damwk1,self.damwv1,self.damwo1,self.damkeysize1=multiheadattention(self.modelsize,self.h)\n        self.damwq2,self.damwk2,self.damwv2,self.damwo2,self.damkeysize2=multiheadattention(self.modelsize,self.h)\n        self.batchnorm1=tf.keras.layers.BatchNormalization()\n        self.midnorm=tf.keras.layers.BatchNormalization()\n        self.dens1=tf.keras.layers.Dense(4*modelsize,activation=tf.nn.relu)\n        self.dens2=tf.keras.layers.Dense(self.modelsize)\n        self.batchnorm2=tf.keras.layers.BatchNormalization()\n        self.dens3=tf.keras.layers.Dense(len(headwordind))\n        self.batchnorm3=tf.keras.layers.BatchNormalization()\n    def call(self,encoderoutput,yinput):\n        deemb=[]\n        for i in range(yinput.shape[1]):\n            dmb=tf.nn.embedding_lookup(self.dembedvar,tf.expand_dims(yinput[:,i],axis=1))\n            deemb.append(dmb)\n    \n    \n        deemb=tf.concat(deemb,axis=1)\n    \n        botsubin=deemb\n    \n        botsubout1=[]\n        for j in range(botsubin.shape[1]):\n            values=botsubin[:,j,:]\n            attention=call(tf.expand_dims(botsubin[:,j,:],axis=1),botsubin[:,:j,:],self.dkeysize1,self.h,self.dwq1,self.dwk1,self.dwv1,self.dwo1)\n            botsubout1.append(attention)\n\n        botsubout1=tf.concat(botsubout1,axis=1)\n        botsubout1=botsubout1+botsubin\n        botsubout1=self.batchnorm1(botsubout1)\n    \n    \n        midsubin=botsubout1\n    \n        midsubout=[]\n        for j in range(midsubin.shape[1]):\n            datt=call(tf.expand_dims(midsubin[:,j,:],axis=1),encoderoutput,self.damkeysize1,self.h,self.damwq1,self.damwk1,self.damwv1,self.damwo1)\n            midsubout.append(datt)\n    \n    \n        midsubout1=tf.concat(midsubout,axis=1)\n        midsubout11=midsubout1+midsubin\n    \n        midsubout12=self.batchnorm2(midsubout11)\n    \n        dffin=midsubout12\n        dffout=self.dens1(dffin)\n        dffout=self.dens2(dffout)\n    \n        dffout=dffout+dffin\n        dffout=self.batchnorm3(dffout)\n    \n        logits1=self.dens3(dffout)\n    \n        return logits1    ","execution_count":61,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important Realizations while creating Decoder \n# 1:- Create class of decoder  with all variables are defined in init function \n# 2:- It is very important to note that in call function of decoder there shouldnt be any new variable that is created as we need trained variables for prediction later on\n# 3:- Input to decoder should be <start> token + remaining sentence \n# 4:- Output to decoder should be sentence +<end token>\n# 5:- We training decoder in such a way that given previous word predict the next word of sequence \n","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask=tf.where(tf.e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder1=decoder(modelsize,h)","execution_count":63,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logits1=decoder1.call(encoderoutput,yinp)","execution_count":64,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits1,labels=tf.one_hot(yout,len(headwordind))))","execution_count":65,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainoptimizer=tf.train.AdamOptimizer(learning_rate=0.001)\ntrainstep=trainoptimizer.minimize(loss)","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sess=tf.InteractiveSession()","execution_count":67,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sess.run(tf.global_variables_initializer())","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batchsize=64","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(texttoken)","execution_count":70,"outputs":[{"output_type":"execute_result","execution_count":70,"data":{"text/plain":"30000"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"count=0\ntrainloss=[]","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(40):\n    index=np.arange(30000)\n    np.random.shuffle(index)\n    for j in range(0,30000,100):\n        ind=index[j:j+100]\n        \n        xinp=texttokenpad[ind]\n        yinp1=headlineinpad[ind]\n        yout1=headlineoutpad[ind]\n    \n        #print(xinp.shape)\n        #print(yinp1.shape)\n        #print(yout1.shape)\n        _=sess.run(trainstep,feed_dict={x:xinp,yinp:yinp1,yout:yout1})\n        loss1=loss.eval({x:xinp,yinp:yinp1,yout:yout1})\n        count+=1\n        trainloss.append(loss1)\n        if count%20==0:\n            print(\"Epoch:-\",count,\" and loss is :-\",loss1)","execution_count":88,"outputs":[{"output_type":"stream","text":"Epoch:- 6020  and loss is :- 0.47588426\nEpoch:- 6040  and loss is :- 0.43246567\nEpoch:- 6060  and loss is :- 0.43995792\nEpoch:- 6080  and loss is :- 0.5977981\nEpoch:- 6100  and loss is :- 0.552273\nEpoch:- 6120  and loss is :- 0.5455774\nEpoch:- 6140  and loss is :- 0.59062165\nEpoch:- 6160  and loss is :- 0.5655491\nEpoch:- 6180  and loss is :- 0.61066014\nEpoch:- 6200  and loss is :- 0.62031394\nEpoch:- 6220  and loss is :- 0.53284776\nEpoch:- 6240  and loss is :- 0.62387073\nEpoch:- 6260  and loss is :- 0.643402\nEpoch:- 6280  and loss is :- 0.6845625\nEpoch:- 6300  and loss is :- 0.673398\nEpoch:- 6320  and loss is :- 0.4819197\nEpoch:- 6340  and loss is :- 0.5670121\nEpoch:- 6360  and loss is :- 0.50007564\nEpoch:- 6380  and loss is :- 0.507994\nEpoch:- 6400  and loss is :- 0.5213804\nEpoch:- 6420  and loss is :- 0.5020732\nEpoch:- 6440  and loss is :- 0.4571805\nEpoch:- 6460  and loss is :- 0.51267356\nEpoch:- 6480  and loss is :- 0.6028426\nEpoch:- 6500  and loss is :- 0.50957054\nEpoch:- 6520  and loss is :- 0.59237677\nEpoch:- 6540  and loss is :- 0.58747536\nEpoch:- 6560  and loss is :- 0.613278\nEpoch:- 6580  and loss is :- 0.5689179\nEpoch:- 6600  and loss is :- 0.5798327\nEpoch:- 6620  and loss is :- 0.48671907\nEpoch:- 6640  and loss is :- 0.4095994\nEpoch:- 6660  and loss is :- 0.45915136\nEpoch:- 6680  and loss is :- 0.47415704\nEpoch:- 6700  and loss is :- 0.49915177\nEpoch:- 6720  and loss is :- 0.5475974\nEpoch:- 6740  and loss is :- 0.5284065\nEpoch:- 6760  and loss is :- 0.48583892\nEpoch:- 6780  and loss is :- 0.5923046\nEpoch:- 6800  and loss is :- 0.528593\nEpoch:- 6820  and loss is :- 0.53361255\nEpoch:- 6840  and loss is :- 0.5339089\nEpoch:- 6860  and loss is :- 0.51435846\nEpoch:- 6880  and loss is :- 0.53372777\nEpoch:- 6900  and loss is :- 0.60320926\nEpoch:- 6920  and loss is :- 0.36550048\nEpoch:- 6940  and loss is :- 0.40746877\nEpoch:- 6960  and loss is :- 0.42588562\nEpoch:- 6980  and loss is :- 0.41715723\nEpoch:- 7000  and loss is :- 0.38106897\nEpoch:- 7020  and loss is :- 0.42358923\nEpoch:- 7040  and loss is :- 0.43605906\nEpoch:- 7060  and loss is :- 0.44345194\nEpoch:- 7080  and loss is :- 0.49110353\nEpoch:- 7100  and loss is :- 0.47201237\nEpoch:- 7120  and loss is :- 0.5233179\nEpoch:- 7140  and loss is :- 0.4987847\nEpoch:- 7160  and loss is :- 0.43144333\nEpoch:- 7180  and loss is :- 0.55524474\nEpoch:- 7200  and loss is :- 0.5143211\nEpoch:- 7220  and loss is :- 0.37368166\nEpoch:- 7240  and loss is :- 0.3678955\nEpoch:- 7260  and loss is :- 0.35806164\nEpoch:- 7280  and loss is :- 0.3907732\nEpoch:- 7300  and loss is :- 0.37163273\nEpoch:- 7320  and loss is :- 0.40255418\nEpoch:- 7340  and loss is :- 0.45063883\nEpoch:- 7360  and loss is :- 0.39547858\nEpoch:- 7380  and loss is :- 0.46412584\nEpoch:- 7400  and loss is :- 0.410606\nEpoch:- 7420  and loss is :- 0.49927634\nEpoch:- 7440  and loss is :- 0.43672684\nEpoch:- 7460  and loss is :- 0.46050808\nEpoch:- 7480  and loss is :- 0.4811999\nEpoch:- 7500  and loss is :- 0.4522137\nEpoch:- 7520  and loss is :- 0.3302965\nEpoch:- 7540  and loss is :- 0.37391564\nEpoch:- 7560  and loss is :- 0.39465076\nEpoch:- 7580  and loss is :- 0.3643383\nEpoch:- 7600  and loss is :- 0.32576552\nEpoch:- 7620  and loss is :- 0.42036206\nEpoch:- 7640  and loss is :- 0.43932116\nEpoch:- 7660  and loss is :- 0.39636338\nEpoch:- 7680  and loss is :- 0.42882568\nEpoch:- 7700  and loss is :- 0.45488474\nEpoch:- 7720  and loss is :- 0.4028173\nEpoch:- 7740  and loss is :- 0.45219794\nEpoch:- 7760  and loss is :- 0.45165685\nEpoch:- 7780  and loss is :- 0.42344192\nEpoch:- 7800  and loss is :- 0.45410663\nEpoch:- 7820  and loss is :- 0.32422987\nEpoch:- 7840  and loss is :- 0.30002585\nEpoch:- 7860  and loss is :- 0.3083814\nEpoch:- 7880  and loss is :- 0.30407998\nEpoch:- 7900  and loss is :- 0.30976555\nEpoch:- 7920  and loss is :- 0.35474613\nEpoch:- 7940  and loss is :- 0.3812257\nEpoch:- 7960  and loss is :- 0.32123002\nEpoch:- 7980  and loss is :- 0.32293227\nEpoch:- 8000  and loss is :- 0.43980598\nEpoch:- 8020  and loss is :- 0.37964737\nEpoch:- 8040  and loss is :- 0.45620322\nEpoch:- 8060  and loss is :- 0.4853993\nEpoch:- 8080  and loss is :- 0.4521235\nEpoch:- 8100  and loss is :- 0.4498077\nEpoch:- 8120  and loss is :- 0.36561\nEpoch:- 8140  and loss is :- 0.2907275\nEpoch:- 8160  and loss is :- 0.27134654\nEpoch:- 8180  and loss is :- 0.29764712\nEpoch:- 8200  and loss is :- 0.3524431\nEpoch:- 8220  and loss is :- 0.334224\nEpoch:- 8240  and loss is :- 0.32664227\nEpoch:- 8260  and loss is :- 0.37920505\nEpoch:- 8280  and loss is :- 0.43642423\nEpoch:- 8300  and loss is :- 0.35863894\nEpoch:- 8320  and loss is :- 0.3523465\nEpoch:- 8340  and loss is :- 0.36290643\nEpoch:- 8360  and loss is :- 0.4031262\nEpoch:- 8380  and loss is :- 0.37089276\nEpoch:- 8400  and loss is :- 0.47423652\nEpoch:- 8420  and loss is :- 0.26850563\nEpoch:- 8440  and loss is :- 0.2630482\nEpoch:- 8460  and loss is :- 0.25147036\nEpoch:- 8480  and loss is :- 0.2854389\nEpoch:- 8500  and loss is :- 0.30627584\nEpoch:- 8520  and loss is :- 0.31297725\nEpoch:- 8540  and loss is :- 0.31805944\nEpoch:- 8560  and loss is :- 0.2911949\nEpoch:- 8580  and loss is :- 0.32667965\nEpoch:- 8600  and loss is :- 0.2893598\nEpoch:- 8620  and loss is :- 0.3638918\nEpoch:- 8640  and loss is :- 0.38209918\nEpoch:- 8660  and loss is :- 0.38494557\nEpoch:- 8680  and loss is :- 0.41464147\nEpoch:- 8700  and loss is :- 0.42599225\nEpoch:- 8720  and loss is :- 0.26998317\nEpoch:- 8740  and loss is :- 0.21961688\nEpoch:- 8760  and loss is :- 0.25603667\nEpoch:- 8780  and loss is :- 0.26338053\nEpoch:- 8800  and loss is :- 0.2876751\nEpoch:- 8820  and loss is :- 0.24124892\nEpoch:- 8840  and loss is :- 0.30078623\nEpoch:- 8860  and loss is :- 0.29163334\nEpoch:- 8880  and loss is :- 0.29465714\nEpoch:- 8900  and loss is :- 0.2918078\nEpoch:- 8920  and loss is :- 0.2909394\nEpoch:- 8940  and loss is :- 0.2936415\nEpoch:- 8960  and loss is :- 0.3234557\nEpoch:- 8980  and loss is :- 0.3299309\nEpoch:- 9000  and loss is :- 0.38927978\nEpoch:- 9020  and loss is :- 0.26639095\nEpoch:- 9040  and loss is :- 0.21074195\nEpoch:- 9060  and loss is :- 0.26669496\nEpoch:- 9080  and loss is :- 0.2642814\nEpoch:- 9100  and loss is :- 0.27788883\nEpoch:- 9120  and loss is :- 0.2642812\nEpoch:- 9140  and loss is :- 0.28847018\nEpoch:- 9160  and loss is :- 0.26806882\nEpoch:- 9180  and loss is :- 0.3022329\nEpoch:- 9200  and loss is :- 0.30796924\nEpoch:- 9220  and loss is :- 0.3061278\nEpoch:- 9240  and loss is :- 0.27685288\nEpoch:- 9260  and loss is :- 0.31491032\nEpoch:- 9280  and loss is :- 0.35432455\nEpoch:- 9300  and loss is :- 0.3371252\nEpoch:- 9320  and loss is :- 0.20739083\nEpoch:- 9340  and loss is :- 0.22163084\nEpoch:- 9360  and loss is :- 0.16889474\nEpoch:- 9380  and loss is :- 0.24338056\nEpoch:- 9400  and loss is :- 0.22805947\nEpoch:- 9420  and loss is :- 0.25566316\nEpoch:- 9440  and loss is :- 0.21444422\nEpoch:- 9460  and loss is :- 0.24797224\nEpoch:- 9480  and loss is :- 0.22967653\nEpoch:- 9500  and loss is :- 0.2909649\nEpoch:- 9520  and loss is :- 0.2905513\nEpoch:- 9540  and loss is :- 0.24491227\nEpoch:- 9560  and loss is :- 0.33068824\nEpoch:- 9580  and loss is :- 0.27282682\nEpoch:- 9600  and loss is :- 0.2913147\nEpoch:- 9620  and loss is :- 0.21786128\nEpoch:- 9640  and loss is :- 0.21313512\nEpoch:- 9660  and loss is :- 0.24353802\nEpoch:- 9680  and loss is :- 0.23045425\nEpoch:- 9700  and loss is :- 0.23697673\nEpoch:- 9720  and loss is :- 0.23173131\nEpoch:- 9740  and loss is :- 0.2380089\nEpoch:- 9760  and loss is :- 0.23066983\nEpoch:- 9780  and loss is :- 0.24072692\nEpoch:- 9800  and loss is :- 0.24049658\nEpoch:- 9820  and loss is :- 0.30000487\nEpoch:- 9840  and loss is :- 0.3246699\nEpoch:- 9860  and loss is :- 0.304481\nEpoch:- 9880  and loss is :- 0.28511158\nEpoch:- 9900  and loss is :- 0.29954472\nEpoch:- 9920  and loss is :- 0.234947\nEpoch:- 9940  and loss is :- 0.22997741\nEpoch:- 9960  and loss is :- 0.20114417\nEpoch:- 9980  and loss is :- 0.20052531\nEpoch:- 10000  and loss is :- 0.21485446\nEpoch:- 10020  and loss is :- 0.1995211\nEpoch:- 10040  and loss is :- 0.19169943\nEpoch:- 10060  and loss is :- 0.23040457\nEpoch:- 10080  and loss is :- 0.24687721\nEpoch:- 10100  and loss is :- 0.21059494\nEpoch:- 10120  and loss is :- 0.28260404\nEpoch:- 10140  and loss is :- 0.2825081\n","name":"stdout"},{"output_type":"stream","text":"Epoch:- 10160  and loss is :- 0.28242365\nEpoch:- 10180  and loss is :- 0.27195218\nEpoch:- 10200  and loss is :- 0.30220634\nEpoch:- 10220  and loss is :- 0.19162849\nEpoch:- 10240  and loss is :- 0.18472114\nEpoch:- 10260  and loss is :- 0.16443712\nEpoch:- 10280  and loss is :- 0.2188825\nEpoch:- 10300  and loss is :- 0.21250257\nEpoch:- 10320  and loss is :- 0.15679152\nEpoch:- 10340  and loss is :- 0.1864817\nEpoch:- 10360  and loss is :- 0.22742674\nEpoch:- 10380  and loss is :- 0.25123695\nEpoch:- 10400  and loss is :- 0.2203814\nEpoch:- 10420  and loss is :- 0.21550156\nEpoch:- 10440  and loss is :- 0.23732519\nEpoch:- 10460  and loss is :- 0.2949013\nEpoch:- 10480  and loss is :- 0.27407315\nEpoch:- 10500  and loss is :- 0.33562282\nEpoch:- 10520  and loss is :- 0.18775143\nEpoch:- 10540  and loss is :- 0.19686064\nEpoch:- 10560  and loss is :- 0.1829896\nEpoch:- 10580  and loss is :- 0.18498777\nEpoch:- 10600  and loss is :- 0.19019337\nEpoch:- 10620  and loss is :- 0.20190668\nEpoch:- 10640  and loss is :- 0.22845414\nEpoch:- 10660  and loss is :- 0.18016866\nEpoch:- 10680  and loss is :- 0.21175875\nEpoch:- 10700  and loss is :- 0.25006562\nEpoch:- 10720  and loss is :- 0.23753744\nEpoch:- 10740  and loss is :- 0.25499037\nEpoch:- 10760  and loss is :- 0.22844328\nEpoch:- 10780  and loss is :- 0.2644603\nEpoch:- 10800  and loss is :- 0.27238238\nEpoch:- 10820  and loss is :- 0.13223648\nEpoch:- 10840  and loss is :- 0.20349367\nEpoch:- 10860  and loss is :- 0.15757926\nEpoch:- 10880  and loss is :- 0.19018196\nEpoch:- 10900  and loss is :- 0.19721833\nEpoch:- 10920  and loss is :- 0.17000015\nEpoch:- 10940  and loss is :- 0.16502002\nEpoch:- 10960  and loss is :- 0.21778096\nEpoch:- 10980  and loss is :- 0.2189902\nEpoch:- 11000  and loss is :- 0.22043698\nEpoch:- 11020  and loss is :- 0.19064175\nEpoch:- 11040  and loss is :- 0.19900107\nEpoch:- 11060  and loss is :- 0.261454\nEpoch:- 11080  and loss is :- 0.24109027\nEpoch:- 11100  and loss is :- 0.2770978\nEpoch:- 11120  and loss is :- 0.14379267\nEpoch:- 11140  and loss is :- 0.17336018\nEpoch:- 11160  and loss is :- 0.1612418\nEpoch:- 11180  and loss is :- 0.16346698\nEpoch:- 11200  and loss is :- 0.1673982\nEpoch:- 11220  and loss is :- 0.17580238\nEpoch:- 11240  and loss is :- 0.19110622\nEpoch:- 11260  and loss is :- 0.17467594\nEpoch:- 11280  and loss is :- 0.17326027\nEpoch:- 11300  and loss is :- 0.17891793\nEpoch:- 11320  and loss is :- 0.20697628\nEpoch:- 11340  and loss is :- 0.19448599\nEpoch:- 11360  and loss is :- 0.22381744\nEpoch:- 11380  and loss is :- 0.22008462\nEpoch:- 11400  and loss is :- 0.21265489\nEpoch:- 11420  and loss is :- 0.14679255\nEpoch:- 11440  and loss is :- 0.14862129\nEpoch:- 11460  and loss is :- 0.124605335\nEpoch:- 11480  and loss is :- 0.13273866\nEpoch:- 11500  and loss is :- 0.15638712\nEpoch:- 11520  and loss is :- 0.14870165\nEpoch:- 11540  and loss is :- 0.15416549\nEpoch:- 11560  and loss is :- 0.16861643\nEpoch:- 11580  and loss is :- 0.17400748\nEpoch:- 11600  and loss is :- 0.2074616\nEpoch:- 11620  and loss is :- 0.18290265\nEpoch:- 11640  and loss is :- 0.18624902\nEpoch:- 11660  and loss is :- 0.21780074\nEpoch:- 11680  and loss is :- 0.2025135\nEpoch:- 11700  and loss is :- 0.20902544\nEpoch:- 11720  and loss is :- 0.16210076\nEpoch:- 11740  and loss is :- 0.15086907\nEpoch:- 11760  and loss is :- 0.13190089\nEpoch:- 11780  and loss is :- 0.14414108\nEpoch:- 11800  and loss is :- 0.15823935\nEpoch:- 11820  and loss is :- 0.19499962\nEpoch:- 11840  and loss is :- 0.14021182\nEpoch:- 11860  and loss is :- 0.18379274\nEpoch:- 11880  and loss is :- 0.1622155\nEpoch:- 11900  and loss is :- 0.17911468\nEpoch:- 11920  and loss is :- 0.20729119\nEpoch:- 11940  and loss is :- 0.1736986\nEpoch:- 11960  and loss is :- 0.18323189\nEpoch:- 11980  and loss is :- 0.23080423\nEpoch:- 12000  and loss is :- 0.19940118\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"headindword=dict([(j,i) for i,j in headwordind.items()])","execution_count":89,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enout=encoderoutput.eval({x:texttokenpad[11].reshape(1,text)})\nenout1=tf.constant(enout)","execution_count":74,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"'Series' object cannot be interpreted as an integer","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-74-ec63c02ac7a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoderoutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtexttokenpad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0menout1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'Series' object cannot be interpreted as an integer"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"texttokenpad[0].shape","execution_count":90,"outputs":[{"output_type":"execute_result","execution_count":90,"data":{"text/plain":"(74,)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(word,enout1):\n    deinput=tf.constant([[headwordind[word]]],dtype=tf.int64)\n    prediction1=decoder1.call(enout1,deinput)\n    firstval=prediction1.eval()\n    word1=headindword[np.argmax(firstval)]\n    return word1","execution_count":91,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"textindword=dict([(j,i) for i,j in textwordind.items()])","execution_count":92,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def printsent(index):\n    enout=encoderoutput.eval({x:texttokenpad[index].reshape(1,texttokenpad.shape[1])})\n    enout1=tf.constant(enout)\n    count=0\n    word='<start>'\n    li=[]\n    while count<15:\n        word=predict(word,enout1)\n        li.append(word)\n        count+=1\n    print(\"predicted sentence:\",\" \".join(li))\n    print(\"----\")\n    print(\"full text:-\",\" \".join([textindword[i] for i in texttokenpad[index]]))\n    print(\"----\")\n    print(\"original summary:-\",\" \".join([headindword[i] for i in headlineoutpad[index]]))\n    ","execution_count":93,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"printsent(23) # After training for 12000 steps and 20 epochs","execution_count":94,"outputs":[{"output_type":"stream","text":"predicted sentence: jcb home with mn with mn with mn with mn with mn with mn with\n----\nfull text:- a man who has been operating jcb machine for decade karnataka <unk> kallakatta took his newlywed bride home in jcb machine decorated with balloons she was scared and refused to sit on it said kallakatta his wife agreed to sit on the machine when kallakatta assured he has been working with the machine every day for years <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n----\noriginal summary:- jcb operator takes wife home in jcb machine after wedding <end> <pad> <pad> <pad> <pad> <pad> <pad>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"printsent(55)","execution_count":95,"outputs":[{"output_type":"stream","text":"predicted sentence: air india flight num hours limit window shatters flight num hours limit window shatters flight\n----\nfull text:- a mumbai kochi air india flight carrying over num passengers was forced to abort takeoff at the last minute following technical problem on saturday air india engineers declared the aircraft unfit for flying after <unk> it for two hours the flight was scheduled to depart at num num am but passengers were accommodated on another flight that departed at num am <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n----\noriginal summary:- air india flight aborts takeoff declared unfit to fly <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"printsent(101)","execution_count":98,"outputs":[{"output_type":"stream","text":"predicted sentence: teaser of raazi released in bipasha of raazi released in bipasha of raazi released in\n----\nfull text:- the teaser of vicky kaushal and yami gautam starrer uri has been released produced by ronnie screwvala company rsvp the film is based on the indian army surgical strikes in pakistan occupied kashmir following the uri terror attack yami will play an intelligence officer while vicky will be playing the lead commander in chief who led the operation of the surgical strikes <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n----\noriginal summary:- teaser of vicky kaushal yami gautam starrer uri released <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"printsent(202)","execution_count":99,"outputs":[{"output_type":"stream","text":"predicted sentence: don oust for lying about what they re not related num false kathua victim they\n----\nfull text:- arjun kapoor sister anshula kapoor while defending their half sisters janhvi and khushi against trolls commented on one of her posts m requesting you to refrain from using abusive language especially towards my sisters do not appreciate it and have therefore deleted your comments she further replied to the troll let please spread joy and good vibes anshula added <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n----\noriginal summary:- don abuse my sisters janhvi khushi half sister anshula <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}