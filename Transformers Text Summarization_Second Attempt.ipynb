{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport nltk\nimport re\nimport os\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport glob\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\nUsing TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/news-summary/news_summary_more.csv\")","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"                                           headlines  \\\n0  upGrad learner switches to career in ML & Al w...   \n1  Delhi techie wins free food from Swiggy for on...   \n2  New Zealand end Rohit Sharma-led India's 12-ma...   \n3  Aegon life iTerm insurance plan helps customer...   \n4  Have known Hirani for yrs, what if MeToo claim...   \n\n                                                text  \n0  Saurav Kant, an alumnus of upGrad and IIIT-B's...  \n1  Kunal Shah's credit card bill payment platform...  \n2  New Zealand defeated India by 8 wickets in the...  \n3  With Aegon Life iTerm Insurance plan, customer...  \n4  Speaking about the sexual harassment allegatio...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>headlines</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Delhi techie wins free food from Swiggy for on...</td>\n      <td>Kunal Shah's credit card bill payment platform...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n      <td>New Zealand defeated India by 8 wickets in the...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Aegon life iTerm insurance plan helps customer...</td>\n      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n      <td>Speaking about the sexual harassment allegatio...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=df.sample(30000)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text=df1.text","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlines=df1.headlines","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text.head()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"17376    A 48-year-old vegetable vendor named Dadarao B...\n21627    The Turkish lira has become more volatile than...\n42000    The Indian Railways on Monday revealed that it...\n4345     Following a phone call with Nelson Mandela in ...\n91536    Former India hockey captain and Rajya Sabha MP...\nName: text, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(text):\n    text=text.lower()\n    text=text.strip()\n    #text=text.replace(\"'s\",\"\",text)\n    text=re.sub(r'[^\\w\\d]',\" \",text)\n    text=re.sub(r'\\d+','num',text)\n    text=re.sub(r\"ยน\",\"\",text)\n    text=re.sub(r\"\\s\\w\\s\",\" \",text)\n    text=re.sub(r\"\\s{2,}\",\" \",text)\n    text=text.strip()\n    return text","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess(text.iloc[3])","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"'following phone call with nelson mandela in june num former uk pm margaret thatcher had told her then foreign affairs adviser that the ex south african president had closed mind according to newly released secret files the uk then ambassador to south africa sir robin renwick had said mandela wasn as intelligent as ex zimbabwe president robert mugabe the files further revealed'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"processtext=text.apply(lambda x:preprocess(x))","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processheadline=headlines.apply(lambda x:preprocess(x))","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processheadline.iloc[0]","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"'man fills num potholes in mumbai in num yrs after son death'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"processtext.iloc[0]","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"'a num year old vegetable vendor named dadarao bilhore has filled in almost num potholes across mumbai over the past three years after his son died in road accident bilhore num year old son was travelling pillion on motorbike which hit pothole in july num also don want anyone else to lose loved one like we have bilhore said'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"textwordcount=Counter()\nfor i in range(len(processtext)):\n    textwordcount.update(processtext.iloc[i].split())","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlinewordcount=Counter()\nfor i in range(len(processheadline)):\n    headlinewordcount.update(processheadline.iloc[i].split())","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len([x for x,i in textwordcount.items() if i>2])","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"23293"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len([x for x,i in headlinewordcount.items() if i>1])","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"12816"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"textokenizer=Tokenizer(num_words=24000,oov_token='<unk>')\n","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"textokenizer.fit_on_texts(processtext)\ntexttoken=textokenizer.texts_to_sequences(processtext)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"textwordind=textokenizer.word_index\ntextwordind=dict([(i,j) for i,j in textwordind.items() if j<=25000])","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"textwordind['<pad>']=0","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headtokenizer=Tokenizer(num_words=12500,oov_token='<unk>')\nheadtokenizer.fit_on_texts(processheadline)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headwordind=headtokenizer.word_index\nheadwordind=dict([(i,j) for i,j in headwordind.items() if j<=13500])","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headwordind['<pad>']=0","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headwordind['<start>']=len(headwordind)\nheadwordind['<end>']=len(headwordind)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlinetoken=headtokenizer.texts_to_sequences(processheadline)","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlinetokenin=[[headwordind['<start>']]+i for i in headlinetoken]","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlinetokenout=[i+[headwordind['<end>']] for i in headlinetoken]","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlinetokenin[:4]","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"[[13501, 18, 8002, 2, 4907, 4, 62, 4, 2, 125, 10, 117, 88],\n [13501, 2849, 8003, 214, 131, 1, 126, 480],\n [13501, 570, 71, 13, 2, 78, 4499, 5, 2, 63, 547, 1],\n [13501, 50, 91, 22, 9742, 1236, 8004, 4908, 279, 2479, 2711, 6071]]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlinetokenout[:4]","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"[[18, 8002, 2, 4907, 4, 62, 4, 2, 125, 10, 117, 88, 13502],\n [2849, 8003, 214, 131, 1, 126, 480, 13502],\n [570, 71, 13, 2, 78, 4499, 5, 2, 63, 547, 1, 13502],\n [50, 91, 22, 9742, 1236, 8004, 4908, 279, 2479, 2711, 6071, 13502]]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"texttokenpad=pad_sequences(texttoken,padding='post')","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlineinpad=pad_sequences(headlinetokenin,padding='post')\nheadlineoutpad=pad_sequences(headlinetokenout,padding='post')","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texttokenpad.shape","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"(30000, 74)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlineinpad.shape","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"(30000, 17)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=tf.placeholder(shape=[None,texttokenpad.shape[1]],dtype=tf.int32)\nyinp=tf.placeholder(shape=[None,headlineinpad.shape[1]],dtype=tf.int32)\nyout=tf.placeholder(shape=[None,headlineoutpad.shape[1]],dtype=tf.int32)","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxtextlen=texttokenpad.shape[1]\nmaxsumlen=headlineinpad.shape[1]","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxtextlen","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"74"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def positional_embedding(pos, model_size):\n    PE = np.zeros((1, model_size))\n    for i in range(model_size):\n        if i % 2 == 0:\n            PE[:,i] = np.sin(pos / 10000 ** (i / model_size))\n        else:\n            PE[:,i] = np.cos(pos / 10000 ** ((i - 1) / model_size))\n    return PE\n\n# max_length = max(len(data_en[0]), len(data_fr_in[0]))\nMODEL_SIZE = 128\n\npes = []\nfor i in range(maxtextlen):\n    pes.append(positional_embedding(i, MODEL_SIZE))\n\npes = np.concatenate(pes, axis=0)\npes = tf.constant(pes, dtype=tf.float32)","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelsize=128\nh=2\nnumlayers=3\n","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def multiheadattention(modelsize,h):\n    querysize=modelsize//h  # querysize and valuesize are to be same \n    valuesize=modelsize//h\n    keysize=modelsize//h\n    wq=[tf.keras.layers.Dense(querysize,activation='relu') for _ in range(h)]\n    wk=[tf.keras.layers.Dense(querysize,activation='relu') for _ in range(h)]\n    wv=[tf.keras.layers.Dense(querysize,activation='relu') for _ in range(h)]\n    wo=tf.keras.layers.Dense(modelsize)\n    return wq,wk,wv,wo,keysize\n\n","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def call(query,value,keysize,h,wq,wk,wv,wo):\n    # query len and keylen can be different size \n    # they are same only in encoder but for decoder they will be different\n    # keylen is same as value len in decoder\n    heads=[]\n    for i in range(h):\n        score=tf.matmul(wq[i](query),wk[i](value),transpose_b=True)\n        score/=tf.math.sqrt(tf.dtypes.cast(keysize,tf.float32)) # batch x querylen x keylen\n        alignment=tf.nn.softmax(score,axis=2)\n        head=tf.matmul(alignment,wv[i](value))\n        heads.append(head)\n    \n    heads=tf.concat(heads,axis=2) # adds up valuesize dims which here is 64 across 2 dim to get 128\n    # batch x valuelen x 128\n    heads=wo(heads)\n    return heads\n    ","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedvar=tf.Variable(tf.random_normal(shape=[len(textwordind),modelsize]))\n","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wq,wk,wv,wo,keysize=multiheadattention(modelsize,h)","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedvar","execution_count":44,"outputs":[{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"<tf.Variable 'Variable:0' shape=(25001, 128) dtype=float32_ref>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pes","execution_count":45,"outputs":[{"output_type":"execute_result","execution_count":45,"data":{"text/plain":"<tf.Tensor 'Const:0' shape=(74, 128) dtype=float32>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub1=[]\nfor i in range(x.shape[1]):\n    embed=tf.nn.embedding_lookup(embedvar,tf.expand_dims(x[:,i],axis=1))\n    #print(embed.get_shape())\n    sub1.append(embed+pes[i,:])\n\nsubconcat1=tf.concat(sub1,axis=1)","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since we have to add positional embeddings to embed variable thats why we are partioning x variable along sequence length\n# and then adding positional encoding to embeddings","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subconcat1","execution_count":48,"outputs":[{"output_type":"execute_result","execution_count":48,"data":{"text/plain":"<tf.Tensor 'concat:0' shape=(?, 74, 128) dtype=float32>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"subout=[]\nfor j in range(subconcat1.shape[1]):\n    attention1=call(tf.expand_dims(subconcat1[:,j,:],axis=1),subconcat1,keysize,h,wq,wk,wv,wo)\n    subout.append(attention1)\n","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subout=tf.concat(subout,axis=1)","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#One thing we can try may be here call(subconcat1,subconcat1.keysize,h,wq,wk,wv,wo)\n","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subout=subout+subconcat1","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subout=tf.keras.layers.BatchNormalization()(subout)\nffin=tf.keras.layers.Dense(4*modelsize,activation='relu')(subout)\nffin=tf.keras.layers.Dense(modelsize)(ffin)","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ffout=ffin+subout\nffoutnorm=tf.keras.layers.BatchNormalization()(ffout)","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wq2,wk2,wv2,wo2,keysize2=multiheadattention(modelsize,h)","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subin2=ffoutnorm\nsubout2=[]\nfor j in range(subconcat1.shape[1]):\n    attention2=call( tf.expand_dims(subin2[:,j,:],axis=1),subin2,keysize2,h,wq2,wk2,wv2,wo2)\n    subout2.append(attention2)\nsubout2=tf.concat(subout2,axis=1)\n    ","execution_count":56,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subout2=subout2+subin2\nsubout2=tf.keras.layers.BatchNormalization()(subout2)\nffin2 = tf.keras.layers.Dense(4*modelsize,activation=tf.nn.relu)(subout2)\nffin2 = tf.keras.layers.Dense(modelsize)(ffin2)","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ffout2=ffin2+subout2\nffoutnorm2=tf.keras.layers.BatchNormalization()(ffout2)","execution_count":58,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoderoutput=ffoutnorm2","execution_count":59,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoderoutput","execution_count":60,"outputs":[{"output_type":"execute_result","execution_count":60,"data":{"text/plain":"<tf.Tensor 'batch_normalization_3/batchnorm/add_1:0' shape=(?, 74, 128) dtype=float32>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class decoder:\n    def __init__(self,modelsize,h):\n        \n        self.modelsize=modelsize\n        self.h=h\n        self.dembedvar=tf.Variable(tf.random_normal(shape=(len(headwordind),self.modelsize)))\n        self.dwq1,self.dwk1,self.dwv1,self.dwo1,self.dkeysize1=multiheadattention(self.modelsize,self.h)\n        self.dwq2,self.dwk2,self.dwv2,self.dwo2,self.dkeysize2=multiheadattention(self.modelsize,self.h)\n        self.damwq1,self.damwk1,self.damwv1,self.damwo1,self.damkeysize1=multiheadattention(self.modelsize,self.h)\n        self.damwq2,self.damwk2,self.damwv2,self.damwo2,self.damkeysize2=multiheadattention(self.modelsize,self.h)\n        self.batchnorm1=tf.keras.layers.BatchNormalization()\n        self.midnorm=tf.keras.layers.BatchNormalization()\n        self.dens1=tf.keras.layers.Dense(4*modelsize,activation=tf.nn.relu)\n        self.dens2=tf.keras.layers.Dense(self.modelsize)\n        self.batchnorm2=tf.keras.layers.BatchNormalization()\n        self.dens3=tf.keras.layers.Dense(len(headwordind))\n        self.batchnorm3=tf.keras.layers.BatchNormalization()\n    def call(self,encoderoutput,yinput):\n        deemb=[]\n        for i in range(yinput.shape[1]):\n            dmb=tf.nn.embedding_lookup(self.dembedvar,tf.expand_dims(yinput[:,i],axis=1))\n            deemb.append(dmb)\n    \n    \n        deemb=tf.concat(deemb,axis=1)\n    \n        botsubin=deemb\n    \n        botsubout1=[]\n        for j in range(botsubin.shape[1]):\n            values=botsubin[:,j,:]\n            attention=call(tf.expand_dims(botsubin[:,j,:],axis=1),botsubin[:,:j,:],self.dkeysize1,self.h,self.dwq1,self.dwk1,self.dwv1,self.dwo1)\n            botsubout1.append(attention)\n\n        botsubout1=tf.concat(botsubout1,axis=1)\n        botsubout1=botsubout1+botsubin\n        botsubout1=self.batchnorm1(botsubout1)\n    \n    \n        midsubin=botsubout1\n    \n        midsubout=[]\n        for j in range(midsubin.shape[1]):\n            datt=call(tf.expand_dims(midsubin[:,j,:],axis=1),encoderoutput,self.damkeysize1,self.h,self.damwq1,self.damwk1,self.damwv1,self.damwo1)\n            midsubout.append(datt)\n    \n    \n        midsubout1=tf.concat(midsubout,axis=1)\n        midsubout11=midsubout1+midsubin\n    \n        midsubout12=self.batchnorm2(midsubout11)\n    \n        dffin=midsubout12\n        dffout=self.dens1(dffin)\n        dffout=self.dens2(dffout)\n    \n        dffout=dffout+dffin\n        dffout=self.batchnorm3(dffout)\n    \n        logits1=self.dens3(dffout)\n    \n        return logits1    ","execution_count":61,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important Realizations while creating Decoder \n# 1:- Create class of decoder  with all variables are defined in init function \n# 2:- It is very important to note that in call function of decoder there shouldnt be any new variable that is created as we need trained variables for prediction later on\n# 3:- Input to decoder should be <start> token + remaining sentence \n# 4:- Output to decoder should be sentence +<end token>\n# 5:- We training decoder in such a way that given previous word predict the next word of sequence \n","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder1=decoder(modelsize,h)","execution_count":63,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logits1=decoder1.call(encoderoutput,yinp)","execution_count":64,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits1,labels=tf.one_hot(yout,len(headwordind))))","execution_count":65,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainoptimizer=tf.train.AdamOptimizer(learning_rate=0.001)\ntrainstep=trainoptimizer.minimize(loss)","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sess=tf.InteractiveSession()","execution_count":67,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sess.run(tf.global_variables_initializer())","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batchsize=64","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(texttoken)","execution_count":70,"outputs":[{"output_type":"execute_result","execution_count":70,"data":{"text/plain":"30000"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"count=0\ntrainloss=[]","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(20):\n    index=np.arange(30000)\n    np.random.shuffle(index)\n    for j in range(0,30000,100):\n        ind=index[j:j+100]\n        \n        xinp=texttokenpad[ind]\n        yinp1=headlineinpad[ind]\n        yout1=headlineoutpad[ind]\n    \n        #print(xinp.shape)\n        #print(yinp1.shape)\n        #print(yout1.shape)\n        _=sess.run(trainstep,feed_dict={x:xinp,yinp:yinp1,yout:yout1})\n        loss1=loss.eval({x:xinp,yinp:yinp1,yout:yout1})\n        count+=1\n        trainloss.append(loss1)\n        if count%20==0:\n            print(\"Epoch:-\",count,\" and loss is :-\",loss1)","execution_count":72,"outputs":[{"output_type":"stream","text":"Epoch:- 20  and loss is :- 4.7343926\nEpoch:- 40  and loss is :- 4.551646\nEpoch:- 60  and loss is :- 4.3071337\nEpoch:- 80  and loss is :- 4.4451585\nEpoch:- 100  and loss is :- 4.3262453\nEpoch:- 120  and loss is :- 4.2303386\nEpoch:- 140  and loss is :- 4.224185\nEpoch:- 160  and loss is :- 4.146503\nEpoch:- 180  and loss is :- 4.127599\nEpoch:- 200  and loss is :- 4.02826\nEpoch:- 220  and loss is :- 4.010023\nEpoch:- 240  and loss is :- 3.9344652\nEpoch:- 260  and loss is :- 3.9576147\nEpoch:- 280  and loss is :- 3.7862635\nEpoch:- 300  and loss is :- 3.9367054\nEpoch:- 320  and loss is :- 3.6643043\nEpoch:- 340  and loss is :- 3.754556\nEpoch:- 360  and loss is :- 3.5389647\nEpoch:- 380  and loss is :- 3.5413046\nEpoch:- 400  and loss is :- 3.4745688\nEpoch:- 420  and loss is :- 3.4720492\nEpoch:- 440  and loss is :- 3.5092423\nEpoch:- 460  and loss is :- 3.5079372\nEpoch:- 480  and loss is :- 3.4838758\nEpoch:- 500  and loss is :- 3.3375401\nEpoch:- 520  and loss is :- 3.5352547\nEpoch:- 540  and loss is :- 3.4205177\nEpoch:- 560  and loss is :- 3.495216\nEpoch:- 580  and loss is :- 3.3381395\nEpoch:- 600  and loss is :- 3.3149605\nEpoch:- 620  and loss is :- 2.986212\nEpoch:- 640  and loss is :- 2.914525\nEpoch:- 660  and loss is :- 3.0018861\nEpoch:- 680  and loss is :- 2.918913\nEpoch:- 700  and loss is :- 2.8553805\nEpoch:- 720  and loss is :- 2.95361\nEpoch:- 740  and loss is :- 2.9404414\nEpoch:- 760  and loss is :- 3.1040316\nEpoch:- 780  and loss is :- 2.9975908\nEpoch:- 800  and loss is :- 3.0546722\nEpoch:- 820  and loss is :- 3.052928\nEpoch:- 840  and loss is :- 2.9512162\nEpoch:- 860  and loss is :- 3.0033462\nEpoch:- 880  and loss is :- 3.0800052\nEpoch:- 900  and loss is :- 2.922654\nEpoch:- 920  and loss is :- 2.5889957\nEpoch:- 940  and loss is :- 2.5180106\nEpoch:- 960  and loss is :- 2.6040738\nEpoch:- 980  and loss is :- 2.5897727\nEpoch:- 1000  and loss is :- 2.5967166\nEpoch:- 1020  and loss is :- 2.6180236\nEpoch:- 1040  and loss is :- 2.4564517\nEpoch:- 1060  and loss is :- 2.6206167\nEpoch:- 1080  and loss is :- 2.6956112\nEpoch:- 1100  and loss is :- 2.6392658\nEpoch:- 1120  and loss is :- 2.6569645\nEpoch:- 1140  and loss is :- 2.6747909\nEpoch:- 1160  and loss is :- 2.8313723\nEpoch:- 1180  and loss is :- 2.7341237\nEpoch:- 1200  and loss is :- 2.7857163\nEpoch:- 1220  and loss is :- 2.278119\nEpoch:- 1240  and loss is :- 2.3213694\nEpoch:- 1260  and loss is :- 2.2277687\nEpoch:- 1280  and loss is :- 2.3857288\nEpoch:- 1300  and loss is :- 2.210721\nEpoch:- 1320  and loss is :- 2.2129753\nEpoch:- 1340  and loss is :- 2.3013752\nEpoch:- 1360  and loss is :- 2.4077194\nEpoch:- 1380  and loss is :- 2.4354448\nEpoch:- 1400  and loss is :- 2.4130077\nEpoch:- 1420  and loss is :- 2.5306966\nEpoch:- 1440  and loss is :- 2.4736466\nEpoch:- 1460  and loss is :- 2.5810046\nEpoch:- 1480  and loss is :- 2.3622332\nEpoch:- 1500  and loss is :- 2.4891746\nEpoch:- 1520  and loss is :- 1.9711248\nEpoch:- 1540  and loss is :- 1.8965344\nEpoch:- 1560  and loss is :- 2.0673146\nEpoch:- 1580  and loss is :- 1.9468904\nEpoch:- 1600  and loss is :- 2.164128\nEpoch:- 1620  and loss is :- 2.0341606\nEpoch:- 1640  and loss is :- 2.0785348\nEpoch:- 1660  and loss is :- 2.2082167\nEpoch:- 1680  and loss is :- 2.130569\nEpoch:- 1700  and loss is :- 2.0837502\nEpoch:- 1720  and loss is :- 2.0967445\nEpoch:- 1740  and loss is :- 2.1950443\nEpoch:- 1760  and loss is :- 2.1906161\nEpoch:- 1780  and loss is :- 2.1055903\nEpoch:- 1800  and loss is :- 2.2691948\nEpoch:- 1820  and loss is :- 1.7418954\nEpoch:- 1840  and loss is :- 1.7974547\nEpoch:- 1860  and loss is :- 1.8339102\nEpoch:- 1880  and loss is :- 1.8966262\nEpoch:- 1900  and loss is :- 1.8389032\nEpoch:- 1920  and loss is :- 1.969871\nEpoch:- 1940  and loss is :- 2.0138605\nEpoch:- 1960  and loss is :- 1.9995795\nEpoch:- 1980  and loss is :- 1.9709225\nEpoch:- 2000  and loss is :- 1.9199557\nEpoch:- 2020  and loss is :- 1.9172225\nEpoch:- 2040  and loss is :- 1.9871176\nEpoch:- 2060  and loss is :- 1.9767432\nEpoch:- 2080  and loss is :- 1.9621286\nEpoch:- 2100  and loss is :- 2.1610744\nEpoch:- 2120  and loss is :- 1.5364107\nEpoch:- 2140  and loss is :- 1.6415087\nEpoch:- 2160  and loss is :- 1.5990294\nEpoch:- 2180  and loss is :- 1.6468211\nEpoch:- 2200  and loss is :- 1.5958462\nEpoch:- 2220  and loss is :- 1.7085682\nEpoch:- 2240  and loss is :- 1.7499205\nEpoch:- 2260  and loss is :- 1.7931815\nEpoch:- 2280  and loss is :- 1.7194963\nEpoch:- 2300  and loss is :- 1.7815818\nEpoch:- 2320  and loss is :- 1.8991193\nEpoch:- 2340  and loss is :- 1.9021115\nEpoch:- 2360  and loss is :- 1.8174928\nEpoch:- 2380  and loss is :- 1.9205362\nEpoch:- 2400  and loss is :- 1.9041944\nEpoch:- 2420  and loss is :- 1.4275911\nEpoch:- 2440  and loss is :- 1.4813946\nEpoch:- 2460  and loss is :- 1.4368266\nEpoch:- 2480  and loss is :- 1.6062627\nEpoch:- 2500  and loss is :- 1.4518486\nEpoch:- 2520  and loss is :- 1.5456991\nEpoch:- 2540  and loss is :- 1.5674973\nEpoch:- 2560  and loss is :- 1.5915905\nEpoch:- 2580  and loss is :- 1.5603707\nEpoch:- 2600  and loss is :- 1.6637578\nEpoch:- 2620  and loss is :- 1.6783803\nEpoch:- 2640  and loss is :- 1.547276\nEpoch:- 2660  and loss is :- 1.6417779\nEpoch:- 2680  and loss is :- 1.734821\nEpoch:- 2700  and loss is :- 1.6790494\nEpoch:- 2720  and loss is :- 1.2632186\nEpoch:- 2740  and loss is :- 1.4257631\nEpoch:- 2760  and loss is :- 1.357762\nEpoch:- 2780  and loss is :- 1.3910481\nEpoch:- 2800  and loss is :- 1.3495967\nEpoch:- 2820  and loss is :- 1.346576\nEpoch:- 2840  and loss is :- 1.4734056\nEpoch:- 2860  and loss is :- 1.5231476\nEpoch:- 2880  and loss is :- 1.4966542\nEpoch:- 2900  and loss is :- 1.5923333\nEpoch:- 2920  and loss is :- 1.5636925\nEpoch:- 2940  and loss is :- 1.5146002\nEpoch:- 2960  and loss is :- 1.5801431\nEpoch:- 2980  and loss is :- 1.5665555\nEpoch:- 3000  and loss is :- 1.6049393\nEpoch:- 3020  and loss is :- 1.1574311\nEpoch:- 3040  and loss is :- 1.114542\nEpoch:- 3060  and loss is :- 1.2384475\nEpoch:- 3080  and loss is :- 1.2387582\nEpoch:- 3100  and loss is :- 1.2706596\nEpoch:- 3120  and loss is :- 1.2747216\nEpoch:- 3140  and loss is :- 1.3021586\nEpoch:- 3160  and loss is :- 1.3311601\nEpoch:- 3180  and loss is :- 1.443109\nEpoch:- 3200  and loss is :- 1.3508856\nEpoch:- 3220  and loss is :- 1.3426195\nEpoch:- 3240  and loss is :- 1.4938965\nEpoch:- 3260  and loss is :- 1.4597493\nEpoch:- 3280  and loss is :- 1.3429018\nEpoch:- 3300  and loss is :- 1.467025\nEpoch:- 3320  and loss is :- 0.99790454\nEpoch:- 3340  and loss is :- 1.0991291\nEpoch:- 3360  and loss is :- 1.0646402\nEpoch:- 3380  and loss is :- 1.1406784\nEpoch:- 3400  and loss is :- 1.2343965\nEpoch:- 3420  and loss is :- 1.2182335\nEpoch:- 3440  and loss is :- 1.2544162\nEpoch:- 3460  and loss is :- 1.2545798\nEpoch:- 3480  and loss is :- 1.2161666\nEpoch:- 3500  and loss is :- 1.2803575\nEpoch:- 3520  and loss is :- 1.2040421\nEpoch:- 3540  and loss is :- 1.3216579\nEpoch:- 3560  and loss is :- 1.404593\nEpoch:- 3580  and loss is :- 1.3557628\nEpoch:- 3600  and loss is :- 1.4418143\nEpoch:- 3620  and loss is :- 0.97985363\nEpoch:- 3640  and loss is :- 0.9300631\nEpoch:- 3660  and loss is :- 1.0303667\nEpoch:- 3680  and loss is :- 0.95916736\nEpoch:- 3700  and loss is :- 1.0548329\nEpoch:- 3720  and loss is :- 1.1232115\nEpoch:- 3740  and loss is :- 1.0796168\nEpoch:- 3760  and loss is :- 1.0952075\nEpoch:- 3780  and loss is :- 1.113083\nEpoch:- 3800  and loss is :- 1.1355249\nEpoch:- 3820  and loss is :- 1.2159928\nEpoch:- 3840  and loss is :- 1.2537594\nEpoch:- 3860  and loss is :- 1.1331087\nEpoch:- 3880  and loss is :- 1.2288151\nEpoch:- 3900  and loss is :- 1.2764992\nEpoch:- 3920  and loss is :- 0.84640044\nEpoch:- 3940  and loss is :- 0.9290257\nEpoch:- 3960  and loss is :- 0.9268448\nEpoch:- 3980  and loss is :- 0.9317696\nEpoch:- 4000  and loss is :- 1.0565988\nEpoch:- 4020  and loss is :- 0.9654798\nEpoch:- 4040  and loss is :- 1.020897\nEpoch:- 4060  and loss is :- 0.9598137\nEpoch:- 4080  and loss is :- 1.0765628\nEpoch:- 4100  and loss is :- 1.0960752\nEpoch:- 4120  and loss is :- 1.0938047\nEpoch:- 4140  and loss is :- 1.2199866\nEpoch:- 4160  and loss is :- 1.1811193\nEpoch:- 4180  and loss is :- 1.1105009\nEpoch:- 4200  and loss is :- 1.2351645\nEpoch:- 4220  and loss is :- 0.7842417\nEpoch:- 4240  and loss is :- 0.90721035\nEpoch:- 4260  and loss is :- 0.82430667\n","name":"stdout"},{"output_type":"stream","text":"Epoch:- 4280  and loss is :- 0.8561341\nEpoch:- 4300  and loss is :- 0.952933\nEpoch:- 4320  and loss is :- 0.96230227\nEpoch:- 4340  and loss is :- 0.93125814\nEpoch:- 4360  and loss is :- 0.91142726\nEpoch:- 4380  and loss is :- 0.9744087\nEpoch:- 4400  and loss is :- 0.977631\nEpoch:- 4420  and loss is :- 1.0528629\nEpoch:- 4440  and loss is :- 1.0132817\nEpoch:- 4460  and loss is :- 1.0458969\nEpoch:- 4480  and loss is :- 0.9271162\nEpoch:- 4500  and loss is :- 1.086753\nEpoch:- 4520  and loss is :- 0.82785463\nEpoch:- 4540  and loss is :- 0.7640037\nEpoch:- 4560  and loss is :- 0.7747284\nEpoch:- 4580  and loss is :- 0.7491427\nEpoch:- 4600  and loss is :- 0.83882153\nEpoch:- 4620  and loss is :- 0.8327111\nEpoch:- 4640  and loss is :- 0.7778619\nEpoch:- 4660  and loss is :- 0.91110325\nEpoch:- 4680  and loss is :- 0.8631939\nEpoch:- 4700  and loss is :- 0.9310546\nEpoch:- 4720  and loss is :- 0.9472607\nEpoch:- 4740  and loss is :- 0.81522506\nEpoch:- 4760  and loss is :- 1.0009971\nEpoch:- 4780  and loss is :- 0.89094853\nEpoch:- 4800  and loss is :- 0.84030527\nEpoch:- 4820  and loss is :- 0.71720755\nEpoch:- 4840  and loss is :- 0.7521424\nEpoch:- 4860  and loss is :- 0.62072766\nEpoch:- 4880  and loss is :- 0.57362324\nEpoch:- 4900  and loss is :- 0.7033922\nEpoch:- 4920  and loss is :- 0.6506704\nEpoch:- 4940  and loss is :- 0.7752627\nEpoch:- 4960  and loss is :- 0.724416\nEpoch:- 4980  and loss is :- 0.8053365\nEpoch:- 5000  and loss is :- 0.82594097\nEpoch:- 5020  and loss is :- 0.9280958\nEpoch:- 5040  and loss is :- 0.84466374\nEpoch:- 5060  and loss is :- 0.8574127\nEpoch:- 5080  and loss is :- 0.8597241\nEpoch:- 5100  and loss is :- 0.85231483\nEpoch:- 5120  and loss is :- 0.6662664\nEpoch:- 5140  and loss is :- 0.5882187\nEpoch:- 5160  and loss is :- 0.6613861\nEpoch:- 5180  and loss is :- 0.6760518\nEpoch:- 5200  and loss is :- 0.56788874\nEpoch:- 5220  and loss is :- 0.6490245\nEpoch:- 5240  and loss is :- 0.7310548\nEpoch:- 5260  and loss is :- 0.768998\nEpoch:- 5280  and loss is :- 0.7052859\nEpoch:- 5300  and loss is :- 0.68469065\nEpoch:- 5320  and loss is :- 0.7918077\nEpoch:- 5340  and loss is :- 0.7337099\nEpoch:- 5360  and loss is :- 0.84534115\nEpoch:- 5380  and loss is :- 0.7985032\nEpoch:- 5400  and loss is :- 0.8421693\nEpoch:- 5420  and loss is :- 0.5387403\nEpoch:- 5440  and loss is :- 0.5410695\nEpoch:- 5460  and loss is :- 0.5605497\nEpoch:- 5480  and loss is :- 0.6041188\nEpoch:- 5500  and loss is :- 0.6202972\nEpoch:- 5520  and loss is :- 0.62499994\nEpoch:- 5540  and loss is :- 0.6714082\nEpoch:- 5560  and loss is :- 0.67688715\nEpoch:- 5580  and loss is :- 0.71359706\nEpoch:- 5600  and loss is :- 0.69805276\nEpoch:- 5620  and loss is :- 0.7119208\nEpoch:- 5640  and loss is :- 0.6456577\nEpoch:- 5660  and loss is :- 0.6683552\nEpoch:- 5680  and loss is :- 0.75426626\nEpoch:- 5700  and loss is :- 0.7980097\nEpoch:- 5720  and loss is :- 0.55346334\nEpoch:- 5740  and loss is :- 0.5097651\nEpoch:- 5760  and loss is :- 0.5218737\nEpoch:- 5780  and loss is :- 0.548012\nEpoch:- 5800  and loss is :- 0.56771827\nEpoch:- 5820  and loss is :- 0.549193\nEpoch:- 5840  and loss is :- 0.5943149\nEpoch:- 5860  and loss is :- 0.63123333\nEpoch:- 5880  and loss is :- 0.7083488\nEpoch:- 5900  and loss is :- 0.7099031\nEpoch:- 5920  and loss is :- 0.70524436\nEpoch:- 5940  and loss is :- 0.63797176\nEpoch:- 5960  and loss is :- 0.70301545\nEpoch:- 5980  and loss is :- 0.75067556\nEpoch:- 6000  and loss is :- 0.7054229\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"headindword=dict([(j,i) for i,j in headwordind.items()])","execution_count":75,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enout=encoderoutput.eval({x:texttokenpad[11].reshape(1,text)})\nenout1=tf.constant(enout)","execution_count":74,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"'Series' object cannot be interpreted as an integer","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-74-ec63c02ac7a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoderoutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtexttokenpad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0menout1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'Series' object cannot be interpreted as an integer"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"texttokenpad[0].shape","execution_count":76,"outputs":[{"output_type":"execute_result","execution_count":76,"data":{"text/plain":"(74,)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(word,enout1):\n    deinput=tf.constant([[headwordind[word]]],dtype=tf.int64)\n    prediction1=decoder1.call(enout1,deinput)\n    firstval=prediction1.eval()\n    word1=headindword[np.argmax(firstval)]\n    return word1","execution_count":77,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"textindword=dict([(j,i) for i,j in textwordind.items()])","execution_count":78,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def printsent(index):\n    enout=encoderoutput.eval({x:texttokenpad[index].reshape(1,texttokenpad.shape[1])})\n    enout1=tf.constant(enout)\n    count=0\n    word='<start>'\n    li=[]\n    while count<15:\n        word=predict(word,enout1)\n        li.append(word)\n        count+=1\n    print(\"predicted sentence:\",\" \".join(li))\n    print(\"----\")\n    print(\"full text:-\",\" \".join([textindword[i] for i in texttokenpad[index]]))\n    print(\"----\")\n    print(\"original summary:-\",\" \".join([headindword[i] for i in headlineoutpad[index]]))\n    ","execution_count":79,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"printsent(23) # After training for 3000 stepsand 10 epochs","execution_count":81,"outputs":[{"output_type":"stream","text":"predicted sentence: jcb visit man <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n----\nfull text:- a man who has been operating jcb machine for decade karnataka <unk> kallakatta took his newlywed bride home in jcb machine decorated with balloons she was scared and refused to sit on it said kallakatta his wife agreed to sit on the machine when kallakatta assured he has been working with the machine every day for years <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n----\noriginal summary:- jcb operator takes wife home in jcb machine after wedding <end> <pad> <pad> <pad> <pad> <pad> <pad>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"textindword[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}