{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport nltk\nimport re\nimport os\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport glob\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\nUsing TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/news-summary/news_summary_more.csv\")","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"                                           headlines  \\\n0  upGrad learner switches to career in ML & Al w...   \n1  Delhi techie wins free food from Swiggy for on...   \n2  New Zealand end Rohit Sharma-led India's 12-ma...   \n3  Aegon life iTerm insurance plan helps customer...   \n4  Have known Hirani for yrs, what if MeToo claim...   \n\n                                                text  \n0  Saurav Kant, an alumnus of upGrad and IIIT-B's...  \n1  Kunal Shah's credit card bill payment platform...  \n2  New Zealand defeated India by 8 wickets in the...  \n3  With Aegon Life iTerm Insurance plan, customer...  \n4  Speaking about the sexual harassment allegatio...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>headlines</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Delhi techie wins free food from Swiggy for on...</td>\n      <td>Kunal Shah's credit card bill payment platform...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n      <td>New Zealand defeated India by 8 wickets in the...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Aegon life iTerm insurance plan helps customer...</td>\n      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n      <td>Speaking about the sexual harassment allegatio...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=df.sample(30000)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text=df1.text","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlines=df1.headlines","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text.head()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"32615    A man in Bihar's Nalanda was arrested on Thurs...\n62848    Former Trinamool Congress (TMC) leader Mukul R...\n70555    Former 'Bhabi Ji Ghar Par Hai' actress Shilpa ...\n64803    According to reports, the producers of Vijay-s...\n20826    Technology giant Apple has removed illegal gam...\nName: text, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(text):\n    text=text.lower()\n    text=text.strip()\n    #text=text.replace(\"'s\",\"\",text)\n    text=re.sub(r'[^\\w\\d]',\" \",text)\n    text=re.sub(r'\\d+','num',text)\n    text=re.sub(r\"ยน\",\"\",text)\n    text=re.sub(r\"\\s\\w\\s\",\" \",text)\n    text=re.sub(r\"\\s{2,}\",\" \",text)\n    text=text.strip()\n    return text","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess(text.iloc[3])","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"'according to reports the producers of vijay starrer film mersal have agreed to remove the scenes that mocked gst and digital india this comes after the bharatiya janata party tamil nadu unit objected to the references in the film vijay has starred in triple roles in the film which deals with corruption in india medical industry'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"processtext=text.apply(lambda x:preprocess(x))","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processheadline=headlines.apply(lambda x:preprocess(x))","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processheadline.iloc[0]","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"'bihar man held for raping minor daughter for num months'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"processtext.iloc[0]","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"'a man in bihar nalanda was arrested on thursday for allegedly raping his num year old daughter repeatedly over the past six months the ordeal came to light when the man neighbours caught him trying to commit the crime in his house when he was in an inebriated condition the victim then revealed he had raped her multiple times in the past'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"textwordcount=Counter()\nfor i in range(len(processtext)):\n    textwordcount.update(processtext.iloc[i].split())","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlinewordcount=Counter()\nfor i in range(len(processheadline)):\n    headlinewordcount.update(processheadline.iloc[i].split())","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len([x for x,i in textwordcount.items() if i>2])","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"23190"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len([x for x,i in headlinewordcount.items() if i>1])","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"12790"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"textokenizer=Tokenizer(num_words=24000,oov_token='<unk>')\n","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"textokenizer.fit_on_texts(processtext)\ntexttoken=textokenizer.texts_to_sequences(processtext)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"textwordind=textokenizer.word_index\ntextwordind=dict([(i,j) for i,j in textwordind.items() if j<=25000])","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"textwordind['<pad>']=0","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headtokenizer=Tokenizer(num_words=12500,oov_token='<unk>')\nheadtokenizer.fit_on_texts(processheadline)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headwordind=headtokenizer.word_index\nheadwordind=dict([(i,j) for i,j in headwordind.items() if j<=13500])","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headwordind['<pad>']=0","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headwordind['<start>']=len(headwordind)\nheadwordind['<end>']=len(headwordind)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlinetoken=headtokenizer.texts_to_sequences(processheadline)","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlinetokenin=[[headwordind['<start>']]+i for i in headlinetoken]","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlinetokenout=[i+[headwordind['<end>']] for i in headlinetoken]","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlinetokenin[:4]","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"[[13501, 205, 18, 125, 5, 785, 852, 199, 5, 2, 248],\n [13501, 215, 4178, 97, 113, 4523, 1991, 971, 29],\n [13501, 21, 1734, 4179, 9722, 2681, 352, 7, 114, 4180, 5452],\n [13501, 4181, 3, 818, 3369, 7, 240, 14, 6868, 61]]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlinetokenout[:4]","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"[[205, 18, 125, 5, 785, 852, 199, 5, 2, 248, 13502],\n [215, 4178, 97, 113, 4523, 1991, 971, 29, 13502],\n [21, 1734, 4179, 9722, 2681, 352, 7, 114, 4180, 5452, 13502],\n [4181, 3, 818, 3369, 7, 240, 14, 6868, 61, 13502]]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"texttokenpad=pad_sequences(texttoken,padding='post')","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlineinpad=pad_sequences(headlinetokenin,padding='post')\nheadlineoutpad=pad_sequences(headlinetokenout,padding='post')","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texttokenpad.shape","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"(30000, 77)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"headlineinpad.shape","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"(30000, 17)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=tf.placeholder(shape=[None,texttokenpad.shape[1]],dtype=tf.int32)\nyinp=tf.placeholder(shape=[None,headlineinpad.shape[1]],dtype=tf.int32)\nyout=tf.placeholder(shape=[None,headlineoutpad.shape[1]],dtype=tf.int32)","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxtextlen=texttokenpad.shape[1]\nmaxsumlen=headlineinpad.shape[1]","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxtextlen","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"77"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def positional_embedding(pos, model_size):\n    PE = np.zeros((1, model_size))\n    for i in range(model_size):\n        if i % 2 == 0:\n            PE[:,i] = np.sin(pos / 10000 ** (i / model_size))\n        else:\n            PE[:,i] = np.cos(pos / 10000 ** ((i - 1) / model_size))\n    return PE\n\n# max_length = max(len(data_en[0]), len(data_fr_in[0]))\nMODEL_SIZE = 128\n\npes = []\nfor i in range(maxtextlen):\n    pes.append(positional_embedding(i, MODEL_SIZE))\n\npes = np.concatenate(pes, axis=0)\npes = tf.constant(pes, dtype=tf.float32)","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelsize=128\nh=2\nnumlayers=3\n","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def multiheadattention(modelsize,h):\n    querysize=modelsize//h  # querysize and valuesize are to be same \n    valuesize=modelsize//h\n    keysize=modelsize//h\n    wq=[tf.keras.layers.Dense(querysize,activation='relu') for _ in range(h)]\n    wk=[tf.keras.layers.Dense(querysize,activation='relu') for _ in range(h)]\n    wv=[tf.keras.layers.Dense(querysize,activation='relu') for _ in range(h)]\n    wo=tf.keras.layers.Dense(modelsize)\n    return wq,wk,wv,wo,keysize\n\n","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def call(query,value,keysize,h,wq,wk,wv,wo):\n    # query len and keylen can be different size like in decoder but in encoder they are of same length\n    # they are same only in encoder but for decoder they will be different\n    # keylen is same as value len in decoder\n    heads=[]\n    for i in range(h):\n        score=tf.matmul(wq[i](query),wk[i](value),transpose_b=True)\n        score/=tf.math.sqrt(tf.dtypes.cast(keysize,tf.float32)) # batch x querylen x keylen\n        alignment=tf.nn.softmax(score,axis=2)\n        head=tf.matmul(alignment,wv[i](value))\n        heads.append(head)\n    \n    heads=tf.concat(heads,axis=2) # adds up valuesize dims which here is 64 across 2 dim to get 128\n    # batch x valuelen x 128\n    heads=wo(heads)\n    return heads\n    ","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedvar=tf.Variable(tf.random_normal(shape=[len(textwordind),modelsize]))\n","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wq,wk,wv,wo,keysize=multiheadattention(modelsize,h)","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedvar","execution_count":44,"outputs":[{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"<tf.Variable 'Variable:0' shape=(25001, 128) dtype=float32_ref>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pes","execution_count":45,"outputs":[{"output_type":"execute_result","execution_count":45,"data":{"text/plain":"<tf.Tensor 'Const:0' shape=(77, 128) dtype=float32>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub1=[]\nfor i in range(x.shape[1]):\n    embed=tf.nn.embedding_lookup(embedvar,tf.expand_dims(x[:,i],axis=1))\n    #print(embed.get_shape())\n    sub1.append(embed+pes[i,:])\n\nsubconcat1=tf.concat(sub1,axis=1)","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since we have to add positional embeddings to embed variable thats why we are partioning x variable along sequence length\n# and then adding positional encoding to embeddings","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subconcat1","execution_count":48,"outputs":[{"output_type":"execute_result","execution_count":48,"data":{"text/plain":"<tf.Tensor 'concat:0' shape=(?, 77, 128) dtype=float32>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"subout=[]\nfor j in range(subconcat1.shape[1]):\n    attention1=call(tf.expand_dims(subconcat1[:,j,:],axis=1),subconcat1,keysize,h,wq,wk,wv,wo)\n    subout.append(attention1)\n","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subout=tf.concat(subout,axis=1)","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#One thing we can try may be here call(subconcat1,subconcat1.keysize,h,wq,wk,wv,wo)\n","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subout=subout+subconcat1","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subout=tf.keras.layers.BatchNormalization()(subout)\nffin=tf.keras.layers.Dense(4*modelsize,activation='relu')(subout)\nffin=tf.keras.layers.Dense(modelsize)(ffin)","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ffout=ffin+subout\nffoutnorm=tf.keras.layers.BatchNormalization()(ffout)","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wq2,wk2,wv2,wo2,keysize2=multiheadattention(modelsize,h)","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subin2=ffoutnorm\nsubout2=[]\nfor j in range(subconcat1.shape[1]):\n    attention2=call( tf.expand_dims(subin2[:,j,:],axis=1),subin2,keysize2,h,wq2,wk2,wv2,wo2)\n    subout2.append(attention2)\nsubout2=tf.concat(subout2,axis=1)\n    ","execution_count":56,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subout2=subout2+subin2\nsubout2=tf.keras.layers.BatchNormalization()(subout2)\nffin2 = tf.keras.layers.Dense(4*modelsize,activation=tf.nn.relu)(subout2)\nffin2 = tf.keras.layers.Dense(modelsize)(ffin2)","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ffout2=ffin2+subout2\nffoutnorm2=tf.keras.layers.BatchNormalization()(ffout2)","execution_count":58,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoderoutput=ffoutnorm2","execution_count":59,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoderoutput","execution_count":60,"outputs":[{"output_type":"execute_result","execution_count":60,"data":{"text/plain":"<tf.Tensor 'batch_normalization_3/batchnorm/add_1:0' shape=(?, 77, 128) dtype=float32>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class decoder:\n    def __init__(self,modelsize,h):\n        \n        self.modelsize=modelsize\n        self.h=h\n        self.dembedvar=tf.Variable(tf.random_normal(shape=(len(headwordind),self.modelsize)))\n        self.dwq1,self.dwk1,self.dwv1,self.dwo1,self.dkeysize1=multiheadattention(self.modelsize,self.h)\n        self.dwq2,self.dwk2,self.dwv2,self.dwo2,self.dkeysize2=multiheadattention(self.modelsize,self.h)\n        self.damwq1,self.damwk1,self.damwv1,self.damwo1,self.damkeysize1=multiheadattention(self.modelsize,self.h)\n        self.damwq2,self.damwk2,self.damwv2,self.damwo2,self.damkeysize2=multiheadattention(self.modelsize,self.h)\n        self.batchnorm1=tf.keras.layers.BatchNormalization()\n        self.midnorm=tf.keras.layers.BatchNormalization()\n        self.dens1=tf.keras.layers.Dense(4*modelsize,activation=tf.nn.relu)\n        self.dens2=tf.keras.layers.Dense(self.modelsize)\n        self.batchnorm2=tf.keras.layers.BatchNormalization()\n        self.dens3=tf.keras.layers.Dense(len(headwordind))\n        self.batchnorm3=tf.keras.layers.BatchNormalization()\n    def call(self,encoderoutput,yinput):\n        deemb=[]\n        for i in range(yinput.shape[1]):\n            dmb=tf.nn.embedding_lookup(self.dembedvar,tf.expand_dims(yinput[:,i],axis=1))\n            deemb.append(dmb)\n    \n    \n        deemb=tf.concat(deemb,axis=1)\n    \n        botsubin=deemb\n    \n        botsubout1=[]\n        for j in range(botsubin.shape[1]):\n            values=botsubin[:,j,:]\n            attention=call(tf.expand_dims(botsubin[:,j,:],axis=1),botsubin[:,:j,:],self.dkeysize1,self.h,self.dwq1,self.dwk1,self.dwv1,self.dwo1)\n            botsubout1.append(attention)\n\n        botsubout1=tf.concat(botsubout1,axis=1)\n        botsubout1=botsubout1+botsubin\n        botsubout1=self.batchnorm1(botsubout1)\n    \n    \n        midsubin=botsubout1\n    \n        midsubout=[]\n        for j in range(midsubin.shape[1]):\n            datt=call(tf.expand_dims(midsubin[:,j,:],axis=1),encoderoutput,self.damkeysize1,self.h,self.damwq1,self.damwk1,self.damwv1,self.damwo1)\n            midsubout.append(datt)\n    \n    \n        midsubout1=tf.concat(midsubout,axis=1)\n        midsubout11=midsubout1+midsubin\n    \n        midsubout12=self.batchnorm2(midsubout11)\n    \n        dffin=midsubout12\n        dffout=self.dens1(dffin)\n        dffout=self.dens2(dffout)\n    \n        dffout=dffout+dffin\n        dffout=self.batchnorm3(dffout)\n    \n        logits1=self.dens3(dffout)\n    \n        return logits1    ","execution_count":61,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important Realizations while creating Decoder \n# 1:- Create class of decoder  with all variables are defined in init function \n# 2:- It is very important to note that in call function of decoder there shouldnt be any new variable that is created as we need trained variables for prediction later on\n# 3:- Input to decoder should be <start> token + remaining sentence \n# 4:- Output to decoder should be sentence +<end token>\n# 5:- We training decoder in such a way that given previous word predict the next word of sequence \n","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder1=decoder(modelsize,h)","execution_count":63,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logits1=decoder1.call(encoderoutput,yinp)","execution_count":64,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask=tf.cast(tf.not_equal(yout,0),tf.float32)\n","execution_count":65,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss1=tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits1,labels=tf.one_hot(yout,len(headwordind)))","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss1=loss1*mask   #Masked Loss not taking factor of pad tokens in loss function","execution_count":67,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss=tf.reduce_mean(loss1)","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainoptimizer=tf.train.AdamOptimizer(learning_rate=0.001)\ntrainstep=trainoptimizer.minimize(loss)","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sess=tf.InteractiveSession()","execution_count":70,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sess.run(tf.global_variables_initializer())","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batchsize=64","execution_count":72,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(texttoken)","execution_count":73,"outputs":[{"output_type":"execute_result","execution_count":73,"data":{"text/plain":"30000"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"count=0\ntrainloss=[]","execution_count":74,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(50):\n    index=np.arange(30000)\n    np.random.shuffle(index)\n    for j in range(0,30000,100):\n        ind=index[j:j+100]\n        \n        xinp=texttokenpad[ind]\n        yinp1=headlineinpad[ind]\n        yout1=headlineoutpad[ind]\n    \n        #print(xinp.shape)\n        #print(yinp1.shape)\n        #print(yout1.shape)\n        _=sess.run(trainstep,feed_dict={x:xinp,yinp:yinp1,yout:yout1})\n        loss1=loss.eval({x:xinp,yinp:yinp1,yout:yout1})\n        count+=1\n        trainloss.append(loss1)\n        if count%20==0:\n            print(\"Epoch:-\",count,\" and loss is :-\",loss1)","execution_count":75,"outputs":[{"output_type":"stream","text":"Epoch:- 20  and loss is :- 4.562021\nEpoch:- 40  and loss is :- 4.4963126\nEpoch:- 60  and loss is :- 4.32489\nEpoch:- 80  and loss is :- 4.2585163\nEpoch:- 100  and loss is :- 4.199199\nEpoch:- 120  and loss is :- 4.2190475\nEpoch:- 140  and loss is :- 4.078463\nEpoch:- 160  and loss is :- 4.003566\nEpoch:- 180  and loss is :- 3.952965\nEpoch:- 200  and loss is :- 3.8779852\nEpoch:- 220  and loss is :- 4.022379\nEpoch:- 240  and loss is :- 3.8692722\nEpoch:- 260  and loss is :- 3.8955612\nEpoch:- 280  and loss is :- 3.9406073\nEpoch:- 300  and loss is :- 3.8597453\nEpoch:- 320  and loss is :- 3.589452\nEpoch:- 340  and loss is :- 3.4907882\nEpoch:- 360  and loss is :- 3.452446\nEpoch:- 380  and loss is :- 3.594266\nEpoch:- 400  and loss is :- 3.4805083\nEpoch:- 420  and loss is :- 3.4877813\nEpoch:- 440  and loss is :- 3.3551092\nEpoch:- 460  and loss is :- 3.6186178\nEpoch:- 480  and loss is :- 3.4414103\nEpoch:- 500  and loss is :- 3.3224564\nEpoch:- 520  and loss is :- 3.2939928\nEpoch:- 540  and loss is :- 3.398726\nEpoch:- 560  and loss is :- 3.3425238\nEpoch:- 580  and loss is :- 3.2859669\nEpoch:- 600  and loss is :- 3.2954729\nEpoch:- 620  and loss is :- 2.880408\nEpoch:- 640  and loss is :- 2.989545\nEpoch:- 660  and loss is :- 2.9641953\nEpoch:- 680  and loss is :- 2.8391857\nEpoch:- 700  and loss is :- 2.989002\nEpoch:- 720  and loss is :- 3.0702202\nEpoch:- 740  and loss is :- 2.9818468\nEpoch:- 760  and loss is :- 2.9297714\nEpoch:- 780  and loss is :- 2.8388298\nEpoch:- 800  and loss is :- 2.9775689\nEpoch:- 820  and loss is :- 2.9514148\nEpoch:- 840  and loss is :- 2.9509099\nEpoch:- 860  and loss is :- 2.9774413\nEpoch:- 880  and loss is :- 3.0701232\nEpoch:- 900  and loss is :- 3.0437713\nEpoch:- 920  and loss is :- 2.4689507\nEpoch:- 940  and loss is :- 2.4292696\nEpoch:- 960  and loss is :- 2.5391757\nEpoch:- 980  and loss is :- 2.4184053\nEpoch:- 1000  and loss is :- 2.4483619\nEpoch:- 1020  and loss is :- 2.5167892\nEpoch:- 1040  and loss is :- 2.595086\nEpoch:- 1060  and loss is :- 2.7066593\nEpoch:- 1080  and loss is :- 2.5129478\nEpoch:- 1100  and loss is :- 2.506318\nEpoch:- 1120  and loss is :- 2.5729868\nEpoch:- 1140  and loss is :- 2.5797184\nEpoch:- 1160  and loss is :- 2.6623132\nEpoch:- 1180  and loss is :- 2.7456698\nEpoch:- 1200  and loss is :- 2.743453\nEpoch:- 1220  and loss is :- 2.285175\nEpoch:- 1240  and loss is :- 2.295932\nEpoch:- 1260  and loss is :- 2.1790113\nEpoch:- 1280  and loss is :- 2.269713\nEpoch:- 1300  and loss is :- 2.2367237\nEpoch:- 1320  and loss is :- 2.308636\nEpoch:- 1340  and loss is :- 2.420252\nEpoch:- 1360  and loss is :- 2.2602394\nEpoch:- 1380  and loss is :- 2.2942803\nEpoch:- 1400  and loss is :- 2.3534381\nEpoch:- 1420  and loss is :- 2.3409681\nEpoch:- 1440  and loss is :- 2.3440967\nEpoch:- 1460  and loss is :- 2.402468\nEpoch:- 1480  and loss is :- 2.450501\nEpoch:- 1500  and loss is :- 2.4549541\nEpoch:- 1520  and loss is :- 2.0006506\nEpoch:- 1540  and loss is :- 2.0127628\nEpoch:- 1560  and loss is :- 1.8950342\nEpoch:- 1580  and loss is :- 2.075651\nEpoch:- 1600  and loss is :- 2.002467\nEpoch:- 1620  and loss is :- 2.037731\nEpoch:- 1640  and loss is :- 2.1049752\nEpoch:- 1660  and loss is :- 2.2642996\nEpoch:- 1680  and loss is :- 2.1637661\nEpoch:- 1700  and loss is :- 2.1708853\nEpoch:- 1720  and loss is :- 2.0842483\nEpoch:- 1740  and loss is :- 2.1385257\nEpoch:- 1760  and loss is :- 2.077616\nEpoch:- 1780  and loss is :- 2.3098412\nEpoch:- 1800  and loss is :- 2.199838\nEpoch:- 1820  and loss is :- 1.6168114\nEpoch:- 1840  and loss is :- 1.6459044\nEpoch:- 1860  and loss is :- 1.883231\nEpoch:- 1880  and loss is :- 1.6978747\nEpoch:- 1900  and loss is :- 1.8478899\nEpoch:- 1920  and loss is :- 1.820692\nEpoch:- 1940  and loss is :- 1.8662823\nEpoch:- 1960  and loss is :- 1.9234334\nEpoch:- 1980  and loss is :- 1.914929\nEpoch:- 2000  and loss is :- 1.8909637\nEpoch:- 2020  and loss is :- 1.9951551\nEpoch:- 2040  and loss is :- 1.9071599\nEpoch:- 2060  and loss is :- 1.8046168\nEpoch:- 2080  and loss is :- 1.9531399\nEpoch:- 2100  and loss is :- 2.0019336\nEpoch:- 2120  and loss is :- 1.5188282\nEpoch:- 2140  and loss is :- 1.6213288\nEpoch:- 2160  and loss is :- 1.5786306\nEpoch:- 2180  and loss is :- 1.6807407\nEpoch:- 2200  and loss is :- 1.5853624\nEpoch:- 2220  and loss is :- 1.6825788\nEpoch:- 2240  and loss is :- 1.7025133\nEpoch:- 2260  and loss is :- 1.767278\nEpoch:- 2280  and loss is :- 1.7619656\nEpoch:- 2300  and loss is :- 1.6817079\nEpoch:- 2320  and loss is :- 1.7864721\nEpoch:- 2340  and loss is :- 1.741437\nEpoch:- 2360  and loss is :- 1.7304225\nEpoch:- 2380  and loss is :- 1.8427223\nEpoch:- 2400  and loss is :- 1.7511494\nEpoch:- 2420  and loss is :- 1.3462172\nEpoch:- 2440  and loss is :- 1.4082394\nEpoch:- 2460  and loss is :- 1.4539369\nEpoch:- 2480  and loss is :- 1.3881344\nEpoch:- 2500  and loss is :- 1.5587143\nEpoch:- 2520  and loss is :- 1.4537898\nEpoch:- 2540  and loss is :- 1.4974718\nEpoch:- 2560  and loss is :- 1.6454293\nEpoch:- 2580  and loss is :- 1.6422861\nEpoch:- 2600  and loss is :- 1.6781527\nEpoch:- 2620  and loss is :- 1.6798956\nEpoch:- 2640  and loss is :- 1.6402094\nEpoch:- 2660  and loss is :- 1.7250609\nEpoch:- 2680  and loss is :- 1.6371114\nEpoch:- 2700  and loss is :- 1.6542662\nEpoch:- 2720  and loss is :- 1.1753407\nEpoch:- 2740  and loss is :- 1.2820694\nEpoch:- 2760  and loss is :- 1.2456427\nEpoch:- 2780  and loss is :- 1.3973935\nEpoch:- 2800  and loss is :- 1.3203148\nEpoch:- 2820  and loss is :- 1.3683072\nEpoch:- 2840  and loss is :- 1.519985\nEpoch:- 2860  and loss is :- 1.4717879\nEpoch:- 2880  and loss is :- 1.4500407\nEpoch:- 2900  and loss is :- 1.4302769\nEpoch:- 2920  and loss is :- 1.5143952\nEpoch:- 2940  and loss is :- 1.5049937\nEpoch:- 2960  and loss is :- 1.5407706\nEpoch:- 2980  and loss is :- 1.4800984\nEpoch:- 3000  and loss is :- 1.4999627\nEpoch:- 3020  and loss is :- 1.1392778\nEpoch:- 3040  and loss is :- 1.1685697\nEpoch:- 3060  and loss is :- 1.2287856\nEpoch:- 3080  and loss is :- 1.2307146\nEpoch:- 3100  and loss is :- 1.322899\nEpoch:- 3120  and loss is :- 1.3031068\nEpoch:- 3140  and loss is :- 1.3576955\nEpoch:- 3160  and loss is :- 1.3147194\nEpoch:- 3180  and loss is :- 1.3515024\nEpoch:- 3200  and loss is :- 1.3336831\nEpoch:- 3220  and loss is :- 1.3579118\nEpoch:- 3240  and loss is :- 1.355591\nEpoch:- 3260  and loss is :- 1.424481\nEpoch:- 3280  and loss is :- 1.4373304\nEpoch:- 3300  and loss is :- 1.4693089\nEpoch:- 3320  and loss is :- 1.0725136\nEpoch:- 3340  and loss is :- 1.0616683\nEpoch:- 3360  and loss is :- 1.0143801\nEpoch:- 3380  and loss is :- 1.049458\nEpoch:- 3400  and loss is :- 1.0898805\nEpoch:- 3420  and loss is :- 1.0862654\nEpoch:- 3440  and loss is :- 1.2679373\nEpoch:- 3460  and loss is :- 1.1835439\nEpoch:- 3480  and loss is :- 1.2834557\nEpoch:- 3500  and loss is :- 1.1959269\nEpoch:- 3520  and loss is :- 1.3177994\nEpoch:- 3540  and loss is :- 1.2524545\nEpoch:- 3560  and loss is :- 1.2132744\nEpoch:- 3580  and loss is :- 1.1971772\nEpoch:- 3600  and loss is :- 1.2595035\nEpoch:- 3620  and loss is :- 0.8432346\nEpoch:- 3640  and loss is :- 0.9233763\nEpoch:- 3660  and loss is :- 0.8716843\nEpoch:- 3680  and loss is :- 0.97467315\nEpoch:- 3700  and loss is :- 1.0476818\nEpoch:- 3720  and loss is :- 1.0564598\nEpoch:- 3740  and loss is :- 1.0389788\nEpoch:- 3760  and loss is :- 1.1055533\nEpoch:- 3780  and loss is :- 1.0807203\nEpoch:- 3800  and loss is :- 1.1004562\nEpoch:- 3820  and loss is :- 1.0537593\nEpoch:- 3840  and loss is :- 1.149659\nEpoch:- 3860  and loss is :- 1.1656495\nEpoch:- 3880  and loss is :- 1.1364491\nEpoch:- 3900  and loss is :- 1.2281777\nEpoch:- 3920  and loss is :- 0.8136429\nEpoch:- 3940  and loss is :- 0.86422646\nEpoch:- 3960  and loss is :- 0.85279113\nEpoch:- 3980  and loss is :- 0.9235444\nEpoch:- 4000  and loss is :- 0.94364357\nEpoch:- 4020  and loss is :- 0.9728774\nEpoch:- 4040  and loss is :- 0.92943174\nEpoch:- 4060  and loss is :- 1.0979301\nEpoch:- 4080  and loss is :- 0.9800244\nEpoch:- 4100  and loss is :- 0.94440717\nEpoch:- 4120  and loss is :- 1.0489188\nEpoch:- 4140  and loss is :- 1.023873\nEpoch:- 4160  and loss is :- 0.9623349\nEpoch:- 4180  and loss is :- 1.130461\nEpoch:- 4200  and loss is :- 1.1168591\nEpoch:- 4220  and loss is :- 0.8381513\nEpoch:- 4240  and loss is :- 0.80232024\nEpoch:- 4260  and loss is :- 0.7982951\n","name":"stdout"},{"output_type":"stream","text":"Epoch:- 4280  and loss is :- 0.7525727\nEpoch:- 4300  and loss is :- 0.887455\nEpoch:- 4320  and loss is :- 0.80952215\nEpoch:- 4340  and loss is :- 0.93602496\nEpoch:- 4360  and loss is :- 0.85600555\nEpoch:- 4380  and loss is :- 0.9549904\nEpoch:- 4400  and loss is :- 0.9082549\nEpoch:- 4420  and loss is :- 0.8965307\nEpoch:- 4440  and loss is :- 0.9189729\nEpoch:- 4460  and loss is :- 0.94436437\nEpoch:- 4480  and loss is :- 0.98719805\nEpoch:- 4500  and loss is :- 0.91984165\nEpoch:- 4520  and loss is :- 0.68984306\nEpoch:- 4540  and loss is :- 0.6971966\nEpoch:- 4560  and loss is :- 0.6297551\nEpoch:- 4580  and loss is :- 0.6797754\nEpoch:- 4600  and loss is :- 0.8540136\nEpoch:- 4620  and loss is :- 0.8026871\nEpoch:- 4640  and loss is :- 0.8525636\nEpoch:- 4660  and loss is :- 0.75553346\nEpoch:- 4680  and loss is :- 0.7982793\nEpoch:- 4700  and loss is :- 0.8278764\nEpoch:- 4720  and loss is :- 0.7946691\nEpoch:- 4740  and loss is :- 0.909458\nEpoch:- 4760  and loss is :- 0.8900779\nEpoch:- 4780  and loss is :- 0.856152\nEpoch:- 4800  and loss is :- 0.8867306\nEpoch:- 4820  and loss is :- 0.60809183\nEpoch:- 4840  and loss is :- 0.6016121\nEpoch:- 4860  and loss is :- 0.6538672\nEpoch:- 4880  and loss is :- 0.707297\nEpoch:- 4900  and loss is :- 0.6798677\nEpoch:- 4920  and loss is :- 0.7245719\nEpoch:- 4940  and loss is :- 0.681245\nEpoch:- 4960  and loss is :- 0.7573089\nEpoch:- 4980  and loss is :- 0.79920805\nEpoch:- 5000  and loss is :- 0.80435616\nEpoch:- 5020  and loss is :- 0.8100767\nEpoch:- 5040  and loss is :- 0.6871877\nEpoch:- 5060  and loss is :- 0.8049086\nEpoch:- 5080  and loss is :- 0.88446105\nEpoch:- 5100  and loss is :- 0.795903\nEpoch:- 5120  and loss is :- 0.61279774\nEpoch:- 5140  and loss is :- 0.6552226\nEpoch:- 5160  and loss is :- 0.55085725\nEpoch:- 5180  and loss is :- 0.63866407\nEpoch:- 5200  and loss is :- 0.6222425\nEpoch:- 5220  and loss is :- 0.70246065\nEpoch:- 5240  and loss is :- 0.7856199\nEpoch:- 5260  and loss is :- 0.6918169\nEpoch:- 5280  and loss is :- 0.6602383\nEpoch:- 5300  and loss is :- 0.7047764\nEpoch:- 5320  and loss is :- 0.67142475\nEpoch:- 5340  and loss is :- 0.7139615\nEpoch:- 5360  and loss is :- 0.7796947\nEpoch:- 5380  and loss is :- 0.8307265\nEpoch:- 5400  and loss is :- 0.77887607\nEpoch:- 5420  and loss is :- 0.51343626\nEpoch:- 5440  and loss is :- 0.5043242\nEpoch:- 5460  and loss is :- 0.57141095\nEpoch:- 5480  and loss is :- 0.6238659\nEpoch:- 5500  and loss is :- 0.60314685\nEpoch:- 5520  and loss is :- 0.60622877\nEpoch:- 5540  and loss is :- 0.60039467\nEpoch:- 5560  and loss is :- 0.6779279\nEpoch:- 5580  and loss is :- 0.6569178\nEpoch:- 5600  and loss is :- 0.63536763\nEpoch:- 5620  and loss is :- 0.6954962\nEpoch:- 5640  and loss is :- 0.693544\nEpoch:- 5660  and loss is :- 0.75629497\nEpoch:- 5680  and loss is :- 0.6809583\nEpoch:- 5700  and loss is :- 0.7450579\nEpoch:- 5720  and loss is :- 0.54161924\nEpoch:- 5740  and loss is :- 0.44903037\nEpoch:- 5760  and loss is :- 0.47191805\nEpoch:- 5780  and loss is :- 0.523877\nEpoch:- 5800  and loss is :- 0.59077406\nEpoch:- 5820  and loss is :- 0.5627644\nEpoch:- 5840  and loss is :- 0.5343148\nEpoch:- 5860  and loss is :- 0.61978865\nEpoch:- 5880  and loss is :- 0.5757996\nEpoch:- 5900  and loss is :- 0.63966984\nEpoch:- 5920  and loss is :- 0.65829045\nEpoch:- 5940  and loss is :- 0.6423578\nEpoch:- 5960  and loss is :- 0.6899625\nEpoch:- 5980  and loss is :- 0.62172335\nEpoch:- 6000  and loss is :- 0.68103427\nEpoch:- 6020  and loss is :- 0.47740486\nEpoch:- 6040  and loss is :- 0.47572386\nEpoch:- 6060  and loss is :- 0.43190646\nEpoch:- 6080  and loss is :- 0.49321043\nEpoch:- 6100  and loss is :- 0.47097614\nEpoch:- 6120  and loss is :- 0.5875393\nEpoch:- 6140  and loss is :- 0.5481818\nEpoch:- 6160  and loss is :- 0.50415343\nEpoch:- 6180  and loss is :- 0.53404176\nEpoch:- 6200  and loss is :- 0.59496397\nEpoch:- 6220  and loss is :- 0.5709785\nEpoch:- 6240  and loss is :- 0.5403443\nEpoch:- 6260  and loss is :- 0.64952815\nEpoch:- 6280  and loss is :- 0.5814632\nEpoch:- 6300  and loss is :- 0.5784346\nEpoch:- 6320  and loss is :- 0.38414714\nEpoch:- 6340  and loss is :- 0.45373926\nEpoch:- 6360  and loss is :- 0.4337329\nEpoch:- 6380  and loss is :- 0.46234044\nEpoch:- 6400  and loss is :- 0.5134695\nEpoch:- 6420  and loss is :- 0.4856345\nEpoch:- 6440  and loss is :- 0.5090598\nEpoch:- 6460  and loss is :- 0.52737105\nEpoch:- 6480  and loss is :- 0.5330115\nEpoch:- 6500  and loss is :- 0.5248546\nEpoch:- 6520  and loss is :- 0.5054263\nEpoch:- 6540  and loss is :- 0.5880605\nEpoch:- 6560  and loss is :- 0.5504625\nEpoch:- 6580  and loss is :- 0.53124654\nEpoch:- 6600  and loss is :- 0.669019\nEpoch:- 6620  and loss is :- 0.4136764\nEpoch:- 6640  and loss is :- 0.3821641\nEpoch:- 6660  and loss is :- 0.42491066\nEpoch:- 6680  and loss is :- 0.42337516\nEpoch:- 6700  and loss is :- 0.4274916\nEpoch:- 6720  and loss is :- 0.42622766\nEpoch:- 6740  and loss is :- 0.4748556\nEpoch:- 6760  and loss is :- 0.4689093\nEpoch:- 6780  and loss is :- 0.44889247\nEpoch:- 6800  and loss is :- 0.492167\nEpoch:- 6820  and loss is :- 0.46177134\nEpoch:- 6840  and loss is :- 0.49605203\nEpoch:- 6860  and loss is :- 0.50809723\nEpoch:- 6880  and loss is :- 0.5573337\nEpoch:- 6900  and loss is :- 0.49480447\nEpoch:- 6920  and loss is :- 0.34943825\nEpoch:- 6940  and loss is :- 0.37052783\nEpoch:- 6960  and loss is :- 0.41594815\nEpoch:- 6980  and loss is :- 0.3618374\nEpoch:- 7000  and loss is :- 0.40897596\nEpoch:- 7020  and loss is :- 0.39209574\nEpoch:- 7040  and loss is :- 0.39545482\nEpoch:- 7060  and loss is :- 0.46954626\nEpoch:- 7080  and loss is :- 0.42331323\nEpoch:- 7100  and loss is :- 0.44638294\nEpoch:- 7120  and loss is :- 0.4537723\nEpoch:- 7140  and loss is :- 0.4441716\nEpoch:- 7160  and loss is :- 0.46864316\nEpoch:- 7180  and loss is :- 0.51076716\nEpoch:- 7200  and loss is :- 0.4961977\nEpoch:- 7220  and loss is :- 0.33674312\nEpoch:- 7240  and loss is :- 0.3524567\nEpoch:- 7260  and loss is :- 0.3445454\nEpoch:- 7280  and loss is :- 0.3083658\nEpoch:- 7300  and loss is :- 0.34610957\nEpoch:- 7320  and loss is :- 0.37114322\nEpoch:- 7340  and loss is :- 0.4119768\nEpoch:- 7360  and loss is :- 0.38819066\nEpoch:- 7380  and loss is :- 0.42994958\nEpoch:- 7400  and loss is :- 0.40676168\nEpoch:- 7420  and loss is :- 0.44721213\nEpoch:- 7440  and loss is :- 0.44512224\nEpoch:- 7460  and loss is :- 0.49469784\nEpoch:- 7480  and loss is :- 0.5073696\nEpoch:- 7500  and loss is :- 0.48473245\nEpoch:- 7520  and loss is :- 0.2977592\nEpoch:- 7540  and loss is :- 0.3197648\nEpoch:- 7560  and loss is :- 0.36770108\nEpoch:- 7580  and loss is :- 0.34320444\nEpoch:- 7600  and loss is :- 0.3598351\nEpoch:- 7620  and loss is :- 0.3552718\nEpoch:- 7640  and loss is :- 0.3477892\nEpoch:- 7660  and loss is :- 0.4062682\nEpoch:- 7680  and loss is :- 0.3963725\nEpoch:- 7700  and loss is :- 0.36129633\nEpoch:- 7720  and loss is :- 0.41069558\nEpoch:- 7740  and loss is :- 0.4117475\nEpoch:- 7760  and loss is :- 0.41212264\nEpoch:- 7780  and loss is :- 0.3699946\nEpoch:- 7800  and loss is :- 0.40466464\nEpoch:- 7820  and loss is :- 0.3055637\nEpoch:- 7840  and loss is :- 0.31987143\nEpoch:- 7860  and loss is :- 0.30120528\nEpoch:- 7880  and loss is :- 0.28690526\nEpoch:- 7900  and loss is :- 0.30846566\nEpoch:- 7920  and loss is :- 0.3149416\nEpoch:- 7940  and loss is :- 0.29358318\nEpoch:- 7960  and loss is :- 0.34418303\nEpoch:- 7980  and loss is :- 0.36130273\nEpoch:- 8000  and loss is :- 0.34279868\nEpoch:- 8020  and loss is :- 0.3947491\nEpoch:- 8040  and loss is :- 0.3838954\nEpoch:- 8060  and loss is :- 0.42597038\nEpoch:- 8080  and loss is :- 0.37366056\nEpoch:- 8100  and loss is :- 0.36148357\nEpoch:- 8120  and loss is :- 0.28737834\nEpoch:- 8140  and loss is :- 0.2808765\nEpoch:- 8160  and loss is :- 0.23903623\nEpoch:- 8180  and loss is :- 0.310449\nEpoch:- 8200  and loss is :- 0.3318681\nEpoch:- 8220  and loss is :- 0.3018859\nEpoch:- 8240  and loss is :- 0.28744936\nEpoch:- 8260  and loss is :- 0.37662643\nEpoch:- 8280  and loss is :- 0.34908742\nEpoch:- 8300  and loss is :- 0.32531768\nEpoch:- 8320  and loss is :- 0.31495124\nEpoch:- 8340  and loss is :- 0.39253274\nEpoch:- 8360  and loss is :- 0.3194566\nEpoch:- 8380  and loss is :- 0.39449006\nEpoch:- 8400  and loss is :- 0.38254556\nEpoch:- 8420  and loss is :- 0.2799178\n","name":"stdout"},{"output_type":"stream","text":"Epoch:- 8440  and loss is :- 0.2479976\nEpoch:- 8460  and loss is :- 0.2801419\nEpoch:- 8480  and loss is :- 0.22915578\nEpoch:- 8500  and loss is :- 0.27973706\nEpoch:- 8520  and loss is :- 0.2650887\nEpoch:- 8540  and loss is :- 0.31976938\nEpoch:- 8560  and loss is :- 0.28590706\nEpoch:- 8580  and loss is :- 0.29574922\nEpoch:- 8600  and loss is :- 0.2791296\nEpoch:- 8620  and loss is :- 0.32285118\nEpoch:- 8640  and loss is :- 0.33999914\nEpoch:- 8660  and loss is :- 0.37125188\nEpoch:- 8680  and loss is :- 0.33665434\nEpoch:- 8700  and loss is :- 0.36160108\nEpoch:- 8720  and loss is :- 0.23341896\nEpoch:- 8740  and loss is :- 0.23925218\nEpoch:- 8760  and loss is :- 0.2022391\nEpoch:- 8780  and loss is :- 0.2403394\nEpoch:- 8800  and loss is :- 0.27745017\nEpoch:- 8820  and loss is :- 0.28766185\nEpoch:- 8840  and loss is :- 0.30855128\nEpoch:- 8860  and loss is :- 0.31429794\nEpoch:- 8880  and loss is :- 0.2752699\nEpoch:- 8900  and loss is :- 0.28753227\nEpoch:- 8920  and loss is :- 0.32700503\nEpoch:- 8940  and loss is :- 0.3252495\nEpoch:- 8960  and loss is :- 0.34926108\nEpoch:- 8980  and loss is :- 0.27366114\nEpoch:- 9000  and loss is :- 0.3051276\nEpoch:- 9020  and loss is :- 0.22029993\nEpoch:- 9040  and loss is :- 0.19313578\nEpoch:- 9060  and loss is :- 0.22253446\nEpoch:- 9080  and loss is :- 0.24713533\nEpoch:- 9100  and loss is :- 0.2202467\nEpoch:- 9120  and loss is :- 0.26776144\nEpoch:- 9140  and loss is :- 0.25192493\nEpoch:- 9160  and loss is :- 0.24063037\nEpoch:- 9180  and loss is :- 0.23648188\nEpoch:- 9200  and loss is :- 0.2543775\nEpoch:- 9220  and loss is :- 0.25010905\nEpoch:- 9240  and loss is :- 0.2909042\nEpoch:- 9260  and loss is :- 0.31211817\nEpoch:- 9280  and loss is :- 0.30806386\nEpoch:- 9300  and loss is :- 0.31095827\nEpoch:- 9320  and loss is :- 0.19943555\nEpoch:- 9340  and loss is :- 0.16354972\nEpoch:- 9360  and loss is :- 0.19852844\nEpoch:- 9380  and loss is :- 0.24081223\nEpoch:- 9400  and loss is :- 0.22220543\nEpoch:- 9420  and loss is :- 0.21489617\nEpoch:- 9440  and loss is :- 0.24949588\nEpoch:- 9460  and loss is :- 0.22827937\nEpoch:- 9480  and loss is :- 0.25285783\nEpoch:- 9500  and loss is :- 0.2580968\nEpoch:- 9520  and loss is :- 0.2669894\nEpoch:- 9540  and loss is :- 0.27245724\nEpoch:- 9560  and loss is :- 0.29714376\nEpoch:- 9580  and loss is :- 0.23489219\nEpoch:- 9600  and loss is :- 0.3225517\nEpoch:- 9620  and loss is :- 0.1894363\nEpoch:- 9640  and loss is :- 0.18527289\nEpoch:- 9660  and loss is :- 0.17793363\nEpoch:- 9680  and loss is :- 0.2370768\nEpoch:- 9700  and loss is :- 0.21905313\nEpoch:- 9720  and loss is :- 0.24416047\nEpoch:- 9740  and loss is :- 0.21665166\nEpoch:- 9760  and loss is :- 0.24278021\nEpoch:- 9780  and loss is :- 0.215661\nEpoch:- 9800  and loss is :- 0.23904078\nEpoch:- 9820  and loss is :- 0.3003759\nEpoch:- 9840  and loss is :- 0.26331922\nEpoch:- 9860  and loss is :- 0.27874961\nEpoch:- 9880  and loss is :- 0.28415057\nEpoch:- 9900  and loss is :- 0.30270723\nEpoch:- 9920  and loss is :- 0.15824528\nEpoch:- 9940  and loss is :- 0.18090919\nEpoch:- 9960  and loss is :- 0.18892592\nEpoch:- 9980  and loss is :- 0.16914324\nEpoch:- 10000  and loss is :- 0.17049596\nEpoch:- 10020  and loss is :- 0.18729556\nEpoch:- 10040  and loss is :- 0.20391977\nEpoch:- 10060  and loss is :- 0.20477517\nEpoch:- 10080  and loss is :- 0.22077158\nEpoch:- 10100  and loss is :- 0.22470383\nEpoch:- 10120  and loss is :- 0.22680908\nEpoch:- 10140  and loss is :- 0.24808538\nEpoch:- 10160  and loss is :- 0.28425375\nEpoch:- 10180  and loss is :- 0.25987816\nEpoch:- 10200  and loss is :- 0.28055495\nEpoch:- 10220  and loss is :- 0.13872983\nEpoch:- 10240  and loss is :- 0.17769946\nEpoch:- 10260  and loss is :- 0.1540878\nEpoch:- 10280  and loss is :- 0.16826329\nEpoch:- 10300  and loss is :- 0.1667317\nEpoch:- 10320  and loss is :- 0.16488509\nEpoch:- 10340  and loss is :- 0.18864456\nEpoch:- 10360  and loss is :- 0.19665779\nEpoch:- 10380  and loss is :- 0.19286837\nEpoch:- 10400  and loss is :- 0.26071236\nEpoch:- 10420  and loss is :- 0.22884837\nEpoch:- 10440  and loss is :- 0.18803108\nEpoch:- 10460  and loss is :- 0.21200222\nEpoch:- 10480  and loss is :- 0.19667213\nEpoch:- 10500  and loss is :- 0.24442527\nEpoch:- 10520  and loss is :- 0.16574273\nEpoch:- 10540  and loss is :- 0.17324887\nEpoch:- 10560  and loss is :- 0.15956852\nEpoch:- 10580  and loss is :- 0.1623096\nEpoch:- 10600  and loss is :- 0.18520531\nEpoch:- 10620  and loss is :- 0.18793935\nEpoch:- 10640  and loss is :- 0.16309312\nEpoch:- 10660  and loss is :- 0.19177234\nEpoch:- 10680  and loss is :- 0.18219396\nEpoch:- 10700  and loss is :- 0.19475108\nEpoch:- 10720  and loss is :- 0.24494216\nEpoch:- 10740  and loss is :- 0.20266967\nEpoch:- 10760  and loss is :- 0.20981805\nEpoch:- 10780  and loss is :- 0.21585459\nEpoch:- 10800  and loss is :- 0.2332825\nEpoch:- 10820  and loss is :- 0.13116968\nEpoch:- 10840  and loss is :- 0.16087523\nEpoch:- 10860  and loss is :- 0.13451195\nEpoch:- 10880  and loss is :- 0.12242272\nEpoch:- 10900  and loss is :- 0.16259387\nEpoch:- 10920  and loss is :- 0.18336362\nEpoch:- 10940  and loss is :- 0.18104094\nEpoch:- 10960  and loss is :- 0.17222282\nEpoch:- 10980  and loss is :- 0.19396138\nEpoch:- 11000  and loss is :- 0.1751074\nEpoch:- 11020  and loss is :- 0.20693785\nEpoch:- 11040  and loss is :- 0.21564569\nEpoch:- 11060  and loss is :- 0.24025409\nEpoch:- 11080  and loss is :- 0.20690453\nEpoch:- 11100  and loss is :- 0.20704938\nEpoch:- 11120  and loss is :- 0.14187731\nEpoch:- 11140  and loss is :- 0.1471962\nEpoch:- 11160  and loss is :- 0.116160765\nEpoch:- 11180  and loss is :- 0.12899089\nEpoch:- 11200  and loss is :- 0.15953663\nEpoch:- 11220  and loss is :- 0.181938\nEpoch:- 11240  and loss is :- 0.1538813\nEpoch:- 11260  and loss is :- 0.15589722\nEpoch:- 11280  and loss is :- 0.1857529\nEpoch:- 11300  and loss is :- 0.15261911\nEpoch:- 11320  and loss is :- 0.16788967\nEpoch:- 11340  and loss is :- 0.19792512\nEpoch:- 11360  and loss is :- 0.21617195\nEpoch:- 11380  and loss is :- 0.18217787\nEpoch:- 11400  and loss is :- 0.19077967\nEpoch:- 11420  and loss is :- 0.11462476\nEpoch:- 11440  and loss is :- 0.14224316\nEpoch:- 11460  and loss is :- 0.13664727\nEpoch:- 11480  and loss is :- 0.14874376\nEpoch:- 11500  and loss is :- 0.15573388\nEpoch:- 11520  and loss is :- 0.1416988\nEpoch:- 11540  and loss is :- 0.13516052\nEpoch:- 11560  and loss is :- 0.15982014\nEpoch:- 11580  and loss is :- 0.17024536\nEpoch:- 11600  and loss is :- 0.17163293\nEpoch:- 11620  and loss is :- 0.16037498\nEpoch:- 11640  and loss is :- 0.18861203\nEpoch:- 11660  and loss is :- 0.15944232\nEpoch:- 11680  and loss is :- 0.18315837\nEpoch:- 11700  and loss is :- 0.21601056\nEpoch:- 11720  and loss is :- 0.118976474\nEpoch:- 11740  and loss is :- 0.119913355\nEpoch:- 11760  and loss is :- 0.1311812\nEpoch:- 11780  and loss is :- 0.13559632\nEpoch:- 11800  and loss is :- 0.11607458\nEpoch:- 11820  and loss is :- 0.14667661\nEpoch:- 11840  and loss is :- 0.10847381\nEpoch:- 11860  and loss is :- 0.12691826\nEpoch:- 11880  and loss is :- 0.15657362\nEpoch:- 11900  and loss is :- 0.17002077\nEpoch:- 11920  and loss is :- 0.13473308\nEpoch:- 11940  and loss is :- 0.16677386\nEpoch:- 11960  and loss is :- 0.18752432\nEpoch:- 11980  and loss is :- 0.16335058\nEpoch:- 12000  and loss is :- 0.16632368\nEpoch:- 12020  and loss is :- 0.120328404\nEpoch:- 12040  and loss is :- 0.12897809\nEpoch:- 12060  and loss is :- 0.12463085\nEpoch:- 12080  and loss is :- 0.11081113\nEpoch:- 12100  and loss is :- 0.11833812\nEpoch:- 12120  and loss is :- 0.12276256\nEpoch:- 12140  and loss is :- 0.10402972\nEpoch:- 12160  and loss is :- 0.15645824\nEpoch:- 12180  and loss is :- 0.12692218\nEpoch:- 12200  and loss is :- 0.14294134\nEpoch:- 12220  and loss is :- 0.14336407\nEpoch:- 12240  and loss is :- 0.12408106\nEpoch:- 12260  and loss is :- 0.17440271\nEpoch:- 12280  and loss is :- 0.18934537\nEpoch:- 12300  and loss is :- 0.1686276\nEpoch:- 12320  and loss is :- 0.12392483\nEpoch:- 12340  and loss is :- 0.11162646\nEpoch:- 12360  and loss is :- 0.081789054\nEpoch:- 12380  and loss is :- 0.10431173\nEpoch:- 12400  and loss is :- 0.107317604\nEpoch:- 12420  and loss is :- 0.13268103\nEpoch:- 12440  and loss is :- 0.14217405\nEpoch:- 12460  and loss is :- 0.1496448\nEpoch:- 12480  and loss is :- 0.14189933\n","name":"stdout"},{"output_type":"stream","text":"Epoch:- 12500  and loss is :- 0.16107719\nEpoch:- 12520  and loss is :- 0.1783362\nEpoch:- 12540  and loss is :- 0.17792189\nEpoch:- 12560  and loss is :- 0.14983812\nEpoch:- 12580  and loss is :- 0.17874561\nEpoch:- 12600  and loss is :- 0.17569609\nEpoch:- 12620  and loss is :- 0.10778326\nEpoch:- 12640  and loss is :- 0.0919876\nEpoch:- 12660  and loss is :- 0.0929951\nEpoch:- 12680  and loss is :- 0.10902092\nEpoch:- 12700  and loss is :- 0.124284826\nEpoch:- 12720  and loss is :- 0.108780034\nEpoch:- 12740  and loss is :- 0.13326108\nEpoch:- 12760  and loss is :- 0.13695085\nEpoch:- 12780  and loss is :- 0.14294632\nEpoch:- 12800  and loss is :- 0.14959562\nEpoch:- 12820  and loss is :- 0.16424076\nEpoch:- 12840  and loss is :- 0.14500432\nEpoch:- 12860  and loss is :- 0.1531748\nEpoch:- 12880  and loss is :- 0.18775995\nEpoch:- 12900  and loss is :- 0.12759852\nEpoch:- 12920  and loss is :- 0.10860312\nEpoch:- 12940  and loss is :- 0.10020033\nEpoch:- 12960  and loss is :- 0.12609595\nEpoch:- 12980  and loss is :- 0.10587885\nEpoch:- 13000  and loss is :- 0.11406084\nEpoch:- 13020  and loss is :- 0.1358911\nEpoch:- 13040  and loss is :- 0.12105949\nEpoch:- 13060  and loss is :- 0.11498034\nEpoch:- 13080  and loss is :- 0.126237\nEpoch:- 13100  and loss is :- 0.12375761\nEpoch:- 13120  and loss is :- 0.11315115\nEpoch:- 13140  and loss is :- 0.12629741\nEpoch:- 13160  and loss is :- 0.14356014\nEpoch:- 13180  and loss is :- 0.13534948\nEpoch:- 13200  and loss is :- 0.16687623\nEpoch:- 13220  and loss is :- 0.10308545\nEpoch:- 13240  and loss is :- 0.08072417\nEpoch:- 13260  and loss is :- 0.090788476\nEpoch:- 13280  and loss is :- 0.08970259\nEpoch:- 13300  and loss is :- 0.085031755\nEpoch:- 13320  and loss is :- 0.101020366\nEpoch:- 13340  and loss is :- 0.11626295\nEpoch:- 13360  and loss is :- 0.15178865\nEpoch:- 13380  and loss is :- 0.11066304\nEpoch:- 13400  and loss is :- 0.13262662\nEpoch:- 13420  and loss is :- 0.11310167\nEpoch:- 13440  and loss is :- 0.13558863\nEpoch:- 13460  and loss is :- 0.1393321\nEpoch:- 13480  and loss is :- 0.13135971\nEpoch:- 13500  and loss is :- 0.14124082\nEpoch:- 13520  and loss is :- 0.09706474\nEpoch:- 13540  and loss is :- 0.10117162\nEpoch:- 13560  and loss is :- 0.084974766\nEpoch:- 13580  and loss is :- 0.09098838\nEpoch:- 13600  and loss is :- 0.1073736\nEpoch:- 13620  and loss is :- 0.11278955\nEpoch:- 13640  and loss is :- 0.09298085\nEpoch:- 13660  and loss is :- 0.12011665\nEpoch:- 13680  and loss is :- 0.10316987\nEpoch:- 13700  and loss is :- 0.119988315\nEpoch:- 13720  and loss is :- 0.11563372\nEpoch:- 13740  and loss is :- 0.13439186\nEpoch:- 13760  and loss is :- 0.13827372\nEpoch:- 13780  and loss is :- 0.14789902\nEpoch:- 13800  and loss is :- 0.14177066\nEpoch:- 13820  and loss is :- 0.10613865\nEpoch:- 13840  and loss is :- 0.088591464\nEpoch:- 13860  and loss is :- 0.10228932\nEpoch:- 13880  and loss is :- 0.09774148\nEpoch:- 13900  and loss is :- 0.09301543\nEpoch:- 13920  and loss is :- 0.10600759\nEpoch:- 13940  and loss is :- 0.0932324\nEpoch:- 13960  and loss is :- 0.118946955\nEpoch:- 13980  and loss is :- 0.10320432\nEpoch:- 14000  and loss is :- 0.103613816\nEpoch:- 14020  and loss is :- 0.10231916\nEpoch:- 14040  and loss is :- 0.12911072\nEpoch:- 14060  and loss is :- 0.12766263\nEpoch:- 14080  and loss is :- 0.13536619\nEpoch:- 14100  and loss is :- 0.13150574\nEpoch:- 14120  and loss is :- 0.09385419\nEpoch:- 14140  and loss is :- 0.087395035\nEpoch:- 14160  and loss is :- 0.07625622\nEpoch:- 14180  and loss is :- 0.08560285\nEpoch:- 14200  and loss is :- 0.07854482\nEpoch:- 14220  and loss is :- 0.09585988\nEpoch:- 14240  and loss is :- 0.086816825\nEpoch:- 14260  and loss is :- 0.113438495\nEpoch:- 14280  and loss is :- 0.09864571\nEpoch:- 14300  and loss is :- 0.13735072\nEpoch:- 14320  and loss is :- 0.12035208\nEpoch:- 14340  and loss is :- 0.13113539\nEpoch:- 14360  and loss is :- 0.09570833\nEpoch:- 14380  and loss is :- 0.13649248\nEpoch:- 14400  and loss is :- 0.12766816\nEpoch:- 14420  and loss is :- 0.07568729\nEpoch:- 14440  and loss is :- 0.078104794\nEpoch:- 14460  and loss is :- 0.07753032\nEpoch:- 14480  and loss is :- 0.100908995\nEpoch:- 14500  and loss is :- 0.110555425\nEpoch:- 14520  and loss is :- 0.090937525\nEpoch:- 14540  and loss is :- 0.086032085\nEpoch:- 14560  and loss is :- 0.11576735\nEpoch:- 14580  and loss is :- 0.09261976\nEpoch:- 14600  and loss is :- 0.10277022\nEpoch:- 14620  and loss is :- 0.117010355\nEpoch:- 14640  and loss is :- 0.12342468\nEpoch:- 14660  and loss is :- 0.13457246\nEpoch:- 14680  and loss is :- 0.09912032\nEpoch:- 14700  and loss is :- 0.13152605\nEpoch:- 14720  and loss is :- 0.085410446\nEpoch:- 14740  and loss is :- 0.078300156\nEpoch:- 14760  and loss is :- 0.08123815\nEpoch:- 14780  and loss is :- 0.07881984\nEpoch:- 14800  and loss is :- 0.0793444\nEpoch:- 14820  and loss is :- 0.08842984\nEpoch:- 14840  and loss is :- 0.08137044\nEpoch:- 14860  and loss is :- 0.098740384\nEpoch:- 14880  and loss is :- 0.06959657\nEpoch:- 14900  and loss is :- 0.09074567\nEpoch:- 14920  and loss is :- 0.10613729\nEpoch:- 14940  and loss is :- 0.11544057\nEpoch:- 14960  and loss is :- 0.10670315\nEpoch:- 14980  and loss is :- 0.12145384\nEpoch:- 15000  and loss is :- 0.110089846\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"headindword=dict([(j,i) for i,j in headwordind.items()])","execution_count":85,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texttokenpad[0].shape","execution_count":77,"outputs":[{"output_type":"execute_result","execution_count":77,"data":{"text/plain":"(77,)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(word,enout1):\n    deinput=tf.constant([[headwordind[word]]],dtype=tf.int64)\n    prediction1=decoder1.call(enout1,deinput)\n    firstval=prediction1.eval()\n    word1=headindword[np.argmax(firstval)]\n    return word1","execution_count":78,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"textindword=dict([(j,i) for i,j in textwordind.items()])","execution_count":79,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def printsent(index):\n    enout=encoderoutput.eval({x:texttokenpad[index].reshape(1,texttokenpad.shape[1])})\n    enout1=tf.constant(enout)\n    count=0\n    word='<start>'\n    li=[]\n    while count<15:\n        word=predict(word,enout1)\n        li.append(word)\n        count+=1\n    print(\"predicted sentence:\",\" \".join(li))\n    print(\"----\")\n    print(\"full text:-\",\" \".join([textindword[i] for i in texttokenpad[index]]))\n    print(\"----\")\n    print(\"original summary:-\",\" \".join([headindword[i] for i in headlineoutpad[index]]))\n    ","execution_count":80,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"printsent(23) # After training for 15000 steps and 50 epochs","execution_count":81,"outputs":[{"output_type":"stream","text":"predicted sentence: selena need num crore instagram suffer num crore instagram suffer num crore instagram suffer num\n----\nfull text:- singer selena gomez with earnings of num num crore per post on instagram and over num million followers has topped instagram rich list num by hopper scheduling site for the photo sharing app reality television star kim kardashian ranked second with num num crore per post while football player cristiano ronaldo has claimed the third position on the list <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n----\noriginal summary:- selena tops instagram rich list num with num num cr per post <end> <pad> <pad> <pad> <pad>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"printsent(55)","execution_count":82,"outputs":[{"output_type":"stream","text":"predicted sentence: num injured num injured num injured num injured num injured num injured num injured num\n----\nfull text:- over num num spectators showed up to watch football match being hosted near the line of control loc in north kashmir kupwara district the crowd witnessed num minute match between <unk> sports sopore and <unk> <unk> baramulla such was the enthusiasm of the football fans that for moment we forgot whether we are near the border said the organisers <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n----\noriginal summary:- num num turn up for football match hosted in kashmir near loc <end> <pad> <pad> <pad> <pad>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"printsent(101)","execution_count":83,"outputs":[{"output_type":"stream","text":"predicted sentence: us citizenship for saudi arabia in saudi arabia in saudi arabia in saudi arabia in\n----\nfull text:- the us treasury on thursday imposed sanctions on num saudi officials for their role in the killing of journalist jamal khashoggi at the saudi arabian embassy in turkey istanbul those sanctioned include former top aide to crown prince mohammed bin salman however the sanctions do not target the saudi arabian government an important us security and economic ally <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n----\noriginal summary:- us sanctions num saudis over killing of journo inside embassy <end> <pad> <pad> <pad> <pad> <pad> <pad>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"printsent(20)","execution_count":86,"outputs":[{"output_type":"stream","text":"predicted sentence: i ministry for crore over num crore over num crore over num crore over num\n----\nfull text:- the union information and broadcasting ministry has released num crore for the payment of three months salaries of doordarshan and all india radio employees the amount has been released despite prasar bharati the parent company of dd and air not having signed an agreement to get grants in aid from the government an official said the agreement will be signed subsequently <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n----\noriginal summary:- i ministry releases num cr for salaries of dd air staff <end> <pad> <pad> <pad> <pad> <pad>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}